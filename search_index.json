[
["intro.html", "Multidimensional poverty measurement: A statistical approach with applications Intro", " Multidimensional poverty measurement: A statistical approach with applications Héctor Nájera Intro What is the extent of poverty? Why some population groups or regions are more likely to be poor than others? These two questions are at the hearth of discussions about fairness and social justice. The answer of these questions inform us about some of the effects that our institutions have upon the distribution of resources and unacceptably low living standards. The international consensus is that poverty is multidimensional and that it should be measured accordingly. However, the answer about the extent and nature of multidimensional poverty is contested and unsatisfactory. There are several theoretical and methodological reasons impeding the production of uncontested poverty measures. There are several theories of human needs that debate the substantive aspects that humans must have to live with dignity. Even if this theoretical discussion finds a satisfactory solution, several practical obstacles requite to be addressed for the successful development of poverty scales. These challenges occur at different stages of the production of an index starting with the fact that poverty is a human invention that needs to be tractable using multivariate data to accurately capture its substantive aspects. This turns out to be a noise-magnifying process in that researchers make several assumptions about the relevant set of necessities to include in an index, the thresholds to identify deprivation, the weighting scheme to reflect the importance of different needs, the way in which the poor and not poor are identified. Furthermore, all these assumptions are constrained by the available data, which is seldom collected with the a priori idea of measuring poverty. Invariably, researchers have to make decisions and presumptions which are influenced by biases (plus random error). Therefore, poverty measurement requires a cogent framework to mitigate confirmation biases by putting our assumptions to scrutiny. This book focuses on one of the key problems in contemporary poverty measurement- the lack of a framework to assess the assumptions underlying a multidimensional scale. The book provides a series of tools to fight against prejudice, misconception and error in poverty measurement. It draws upon measurement theory, with a history of 100 years of development, to help researchers to avoid producing noise-magnifying indices. This book provides a series of falsifiable principles and criteria to assess whether our poverty rates are just a reflection of noise mining and unlikely to replicate. Poverty research has overlooked the developments in other fields and although sometimes some aspects of measurement theory are recovered its use is partial, inaccurate and unsystematic. In 2016, the World Bank Commission (2017) on Global Poverty, headed by Sir Anthony Atkinson, has put into perspective the different challenges in multidimensional poverty measurement and set out 21 recommendations. Recommendation 4 of the World Bank report acknowledges the need of validating poverty indices. But it does not propose how to do so. This is understandable because one of the one of the main difficulties in contemporary poverty measurement is the absence of an explicit discussion about how to check all these assumptions. There are several books on poverty measurement but there is none exclusively dedicated to the topic of empirical examination of poverty indices. Most of the books focus on the production of an index but devote little attention to the issue of validation. This reflects the fact that poverty research has followed the path of many other fields. There have been other disciplines in a similar struggle. Educational testing, psychological measurement, sociology but also in the natural sciences biology and medicine often face measurement challenges. However, many of these disciplines have taken measurement very seriously and have adopted a series of practices and principles that reduce uncertainty about the attribute they measure. These areas rely on the seminal work of (Spearman, 1904) on correlation and latent variables that resulted in the development measurement theory and methods with more than 100 years of history and continuous development from the classical works on factor analysis (Cudeck &amp; MacCallum, 2012; Lazardfeld &amp; Henry, 1968; Thorndike &amp; Hagen, 1969; Thurstone, 1947), passing through the development of the principles of validity and reliability (Guttman, 1945, pp. Novick1967, Novick1967, Brennan2006), then through the modern framework of the latent variable approach (Bartholomew, 1987; Kvalheim, 2012; Muthén, 2007; Skrondal &amp; Rabe-Hesketh, 2007) and finally to the classic handbooks that show how all these principles and method constitute sound measurement framework (Allen &amp; Yen, 2001; Brennan, 2006; McDonald, 2013; Michell, 2015; Streiner, Norman, &amp; Cairney, 2015). This framework has been so widely accepted that has lead to the adoption of standards in some academic journals so that authors provide a more objective judgement about the quality of their measurement. The book draws upon measurement theory and methods that have proven to be useful in many other fields, to illustrate how a unified framework can be for empirical examination of multidimensional poverty measures. It translates key concepts and principles of measurement theory and methods and illustrate is implementation using both simulated and real data examples. The book is intended for applied researchers and students. Most of the examples rely on R-software and Mplus (Muthén &amp; Muthén, 2012; R Core Team, 2018). The principal goal of this book is to help researchers, students and technicians at the government to understand the importance of the principles of measurement theory in poverty measurement and to enhance their skills for empirical analyses of poverty indices. After studying the book readers should be able to: Understand why is important to have falsifiable measures in poverty research Identify the difference between a method of aggregation and a methodology for empirical examination Appreciate the relevance of measurement theory to examine poverty indices but also to understand its limitations Understand how the principles of reliability and validity are a necessary condition for a minimum quality of measurement Implement analysis of reliability and validity in widely used software Interpret the results of the analysis critically Appreciate the role measurement invariance and scale equating for the comparison of poverty indices Implement basic analysis of measurement invariance and equating in poverty measurement Identify appropriate and inappropriate uses of the method and principles of measurement theory The book is organised as follows. The first chapter introduces the links between the problems in poverty measurement and the principles of measurement theory. The chapter starts by overweening some of the key debates and consensuses in the literature. It puts emphasis on the challenges and assumptions that take place when measuring poverty and the possible response from measurement theory. The second chapter introduces, discusses and translates the concept of reliability to poverty measurement. It uses simulated data and real data to illustrate the consequences of violating reliability and shows how reliability is deeply connected with some axioms in poverty measurement. The third chapter presents the concept of validity and relates the different types of validity to the checks that can be done in poverty research. Chapter four concerns with the topic of comparability in poverty measurement. It shows how the principle of measurement invariance is central for making valid comparisons across groups and periods. Chapter five continues with the topic of comparability but focuses on the issue of making seemingly incomparable scales comparable. It draws on the principles and method of scale linking and equating. This book would have been possible without the rich discussions with the Bristol team at the Bristol Poverty Institute and the Centre for the Study of Poverty and Social Justice. In particular, thanks to the continues discussion David Gordon, Marco Pomati, Shailen Nandy, etc. The initiative of this book benefit from common concerns with the current state of poverty measurement and the widespread confirmation biases. Here Fernando Cortés (CONEVAL and FLACSO) and Rosa Mar'ia (FLACSO) play a decisive role. Luis Beccaria (pioneer in multidimensional poverty measurement. Universidad General Sarmiento) also gave many valuable ideas. "],
["Chapter-1.html", "Chapter 1 Poverty and measurement theory principles 1.1 The Concept of Poverty 1.2 Theoretical dimensions of poverty 1.3 The measurement of poverty and its challenges 1.4 The poor and the not poor: The poverty line 1.5 A brief on multidimensional poverty measurement", " Chapter 1 Poverty and measurement theory principles Abstract This chapter introduces the concept of poverty and draws upon measurement theory to frame some of the challenges in the production and empirical assessment of poverty indices. The roles of poverty definitions, researcher’s value judgements, desirable properties of poverty indices, survey data and measurement error in the production of poverty measures are described. 1.1 The Concept of Poverty Poverty is one of the capital concepts in social sciences. As such, poverty is a construction of the human mind to depict a state of low living standards. In the realm of social sciences, concepts or constructs are not attribute or feature directly observable using univariate data. Yet, poverty is something that can be grasped intuitively by anyone and, at the same time, a construct with several contested interpretations. This is why is so difficult to measure it because there is a subtle but decisive distinction between direct and indirect observation of a given construct. Poverty has several meanings and part of the difficulty in measuring it has to do with the existence of different definitions. (Spicker, Alvarez, &amp; Gordon, 2006) suggest that the the definitions proposed in the literature can be clustered into three main groups: material (needs, resources and deprivation), economic (living standards, inequality and economic position) and social conditions (entitlements, social security, exclusion, dependence and social class). Spicker et al. (2006), nonetheless, underline the importance of working with scientific definitions of poverty as they meet the standards of the philosophy of science: definitions that are testable so that are falsifiable in a clear way. The contemporary literature the poor are defined in terms of both low living standards and resources. This broad definition suggests the existence of two subpoulation groups that are meaningful and falsifiable in the sense that their profile should predict outcomes that in theory are caused by poverty- mortality, poor health, economic stress, etc. The chief objective of this books is to provide a framework to make poverty indices falsifiable. (Townsend, 1979) argued that poverty can be treated scientifically in that it can be objectively defined and measured. According to his theory, the concept of deprivation was central for the definition of poverty in that it connects command of resources with low living standards. Poverty can be defined as the lack of resources overtime where material and social deprivations are its consequences (Gordon, 2006). This definition does not clarify in what sense lacking something is a standard to classify people as poor or not poor. This is related to the domain that is utilized to identify deprivation and in the literature has to do with the discussion about absolute and relative poverty. Townsend (1987) argued that poverty is relative in the sense that it varies across time and space- the identification of the relevant domain depends on what a society regards as the minimum according to the prevailing living standards. (Sen, 1983) was the most notable thinker against to the idea of poverty as a relative concept. He suggested that poverty was absolute in the sense of not having certain basic opportunities- failure in capabilities. (Altimir, 1979) concluded that Sen and Townsend were talking about two nested thresholds. The absolute core which is universal and a relative one which varies across societies and time. In a series of exchanges in Oxford Economic Papers in the 1980s, Townsend and Sen discussed their views on poverty as an absolute or relative concept. As (Gordon, 2006) points out most of the disagreement is a matter of semantics. (Boltvinik, 1998) provides an recapitulation of the exchanges and he notes that part of the disagreement has to do with the lack of clarity on the space to identify poverty: commodities, resources, capabilities and deprivation. Sen (1983) acknowledged that commodities and characteristics change overtime but to identify poverty researchers must establish when people fail to achieve certain minimum capabilities. There are however, two difficulties in operationalising capabilities. Firts, Thorbecke (2007) points out that measuring capabilities implies observing them ex ante but in practice only outcomes -achieved functionings- can be measured. Second, the is the challenge of specifying the minimum capability set. Whereas some authors have proposed a minimum list (Nussbaum, 2000); others like Sen (2005) and Alkire (2007) have been against this idea. At the core of this debate seems to be a lack of clear distinction between a theoretical list (which can be authoritative if imposed without any sort of evidence) and a backed-up list by some sort of validation via public reasoning or empirical exercises. Measurement of achieved functionings provides the basis to resolve the dispute about absolute-relative definitions of poverty (Spicker et al., 2006). As it implies that capabilities are operationalised through socially defined commodities and characteristics (the next section discusses how this fits a measurement framework that draws on latent variables).1 In the space of outcomes (\\(X\\)), there is very little practical difference between the concepts of achievements and deprivation \\(x\\). However, the question about what are the contents (\\(X\\)) of the definition of poverty remains unanswered at this point. The solution lies in establishing the space of functionings (that relate to deprivation capability in Sen’s terms) or the space of deprivation according to the standards of society in Townsend’s framework. Both authors acknowledged that such space is multidimensional in the sense that it relates to the minimum diverse aspects than enable humans to function/participate in society. 1.2 Theoretical dimensions of poverty Theories of human need, capabilities and relative deprivation have been put forward to frame the (\\(j\\)) dimensions and its contents (\\(x_{ij}\\)) that should be included in both the definition and measure of poverty. Most notably, the Unsatisified Basic Needs approach, with a long track record in Latin America, draws on theories of human need such as those proposed by . Altimir (1979) and Boltvinik &amp; Hernández-Láos (2001) draw upon the concept of human needs (instead of capability or relative deprivation) to define poverty in terms of unmeet basic needs. The UBN approach has had at its core the housing dimension (access to water and sanitation and materials of the dwelling) plus education, food and health deprivation. Boltvinik (2014) reviews the different variants of the UBN and it is possible to appreciate the different dimensions of poverty (as well as diverse aggregation methods and strategies to identify the poor) where he identifies as the improved variant the one that includes time and underpins the Integrated Poverty Measurement Method (IPMM) (Boltvinik, 1992, p. Boltvinik2001). A recently popular variant of the UBN are the hybrid approaches that combine UBN and social rights. Notably, UNICEF’s first international measure of child poverty proposes eight dimensions (Gordon, Nandy, Pantazis, Pemberton, &amp; Townsend, 2003). A similar hybrid variant of the UBN is the Mexican Multidimensional Measure. It has two domains: income and social rights. The human rights domain has five dimensions: Housing, social security and health, education, essential services and food deprivation (Cortés, 2014, p. CONEVAL2011d). The capability-based dimensional models are very similar to the UBN approaches. For example, drawing upon the capability apprach, Klasen (2000) proposes a core deprivation index that it is fairly similar to the standard UBN dimensions. This is undertandable as it focuses on outcomes that often relate to basic human needs. These models are fairly recent and its dimensional structure heavily draw upon the original UBN variants implemented in the 1970s and 1980s in Latin America (Boltvinik, 2014) (Chapter XX discusses some of the aggregation novelties introduced by this approach). The most popular implementation is the UNDP-OPHI international model for acute poverty which classifies the indicators into 3 dimensions: standard of living, education and health (UNDP, 2014). As in the UBN measures the housing facilities and conditions are central for the measure. The implementations in Latin America have put forward a five-dimensional structure: housing, basic services, living standard, education and employment (Santos &amp; Villatoro, 2016). Relative deprivation has also a hierarchical structure but it proposes different dimensions and subdimensions. Townsend (1979)’s original model suggested two main domains: material and social deprivation but with four and seven subdimensions, respectively. For material: dietary, clothing, fuel and light, household facilities and amenities, working conditions, health and educational. For social: Environmental, Family, Recreational and Social (Townsend, 1979, pp. 1173–1174). There have been several models that draw upon relative deprivation but the theory behind them is not as explicit as in the case of the UBN or the Townsend model (Betti, Gagliardi, Lemmi, &amp; Verma, 2015; A.-C. Guio, 2009; Whelan, Nolan, &amp; Maitre, 2006). These nonetheless suggest rather different dimensional structure. A.-C. Guio (2009) proposes a three dimensional model comprising economic strain, enforced lack of durables and housing-related deprivation; Whelan et al. (2006) propose five dimensions (economic strain, consumption, housing facilities, neighbourhood environment, health status) and Betti et al. (2015) put forward seven dimensions that are contain most of Whelan et al. (2006)’s proposal (Chapter X disusses more in detail the implications of these models). Drawing upon this review of the conceptualization of poverty, throughout the book we will be refering to Townsend (1979) definition (Gordon, 2006): Poverty is the lack of command of resources overtime and deprivations its consequence. This concept of poverty is useful because it is simple and is not in tension with many of the conseptual debates described above. The definition acknowledges that Deprivation is multidimensional in that it refers to the unmet needs/functionings that are regarded as essential by society in a given point in time. But, the definition is also useful Therefore, once the set of socially defined needs is established, the question is how the poor is correctly identified? This is the question of this book: What criteria and principles lead to falsify the contents and identification of a poverty scale? To better frame the question is appropriate to review the challenges involved in poverty measurement. 1.3 The measurement of poverty and its challenges The concept of poverty involves raising a series of assumptions about the existence of a minimum living standard that permits meaninfuly to split a population into two groups. Researchers therefore have to make several decisions with regard how to select the dimensions and its indicators but also about how what is the best way to aggregate the information so that the poor is accurately identified (Alkire, 2007; Thorbecke, 2007). These result into a series of decisions that demand theoretical and empirical justification: Specifying the \\(j\\) dimensions Specifying the contents (indicators) of each dimension \\(x_{ij}\\) Deciding the cut off \\(x_{ij}&lt;z\\) of the indicators item Establishing a measurement model about the relationship between dimensions and indicators item Deciding the relative contribution (weights) \\(w_{j}\\) of some indicators/dimensions item Deciding how to aggregate the information to rank the population item Deciding how to split such ranking into two meaningful groups \\(p_k (x_i;z) = 1\\) if \\(c_i\\leq k\\) and \\(p_k (x_i;z) = 0\\) otherwise 1.3.1 Challeges in selection of dimensions, contents, cut offs and weights With regard the first three, Gordon &amp; Nandy (2012)] point out that although there is a consensus that poverty is multidimensional, there is little consensus about the number, contents and nature of the dimensions of poverty. Theoretical frameworks rarely go that far and, although there is some overlap of the dimensions used in different perspectives, there is very little agreement about the content and the nature of the interactions between and within dimensions. Alkire (2007) provides and overview of the different approaches that a researcher can employ to decide the dimensions, indicators and its interaction (public consensus, existing data, convention, deliberative participatory process, data on people’s values). The typical dimensions proposed from the capability, UBN and relative deprivation were overviewed in the previous section. One critical example, in that is grounded on theory but includes an democratic empirical component is the consensual method pioneered by Mack &amp; Lansley (1985). The consensual deprivation approach was a ground-breaking approach in that it linked the theory of relative deprivation with an actual account of the socially perceived needs of the population. More recently, from the capability perspective, other studies have also aimed at finding the necessities of life by asking the population (Clark &amp; Qizilbash, 2005; Narayan, 2001). Although the specification via theory and/or some form of empirical data helps to delimit the space of relevant needs, it does not solve problems about the interaction between indicators and dimensions. More importantly, it does not guarantee that such a list would effectively result in an accurate account of the dimensions of poverty. There are several reasons. One criticism is reflected in the argument pose by McKay (2004) about the differnece between needs and preferences (i.e. How to split between wishes from constraint?). Another critique is about on what basis one decides what is a need from what is not when there is no 100% endorsement (Bradshaw, Holmes, &amp; Hallerod, 1995). Another argument has been about the difficulties to conduct comparative work, i.e. how to compare coutries with different sets of needs. There is, from statistical point of view -which is the topic of this book-, a satisfactory response to these concerns. We will recover these critiques throughout the book as a key task of it is to provide a profound explanation and ilustration of why once some principles of measurement theory are fulfilled, these critiques do not hold. The determination of thresholds levels for the indicators and dimensions is central as it is one of the main sources of variability of an index. Thorbecke (2007) points out that this is often done on a subjective or normative fashion and that it could lead to important discrepancies and reproducibility problems. Aside theory-driven ways to set the thresholds of nominal variables, the consensual approach could be also used to find a split. The best example is the Mexican Multidimensional Measure in that the cut offs where derived from the consensual method and a revision of the norms in Mexico CONEVAL (2011a) (Chapter XX shows how this hybrid approach leads to a more robust measure). Rightly so, Thorbecke (2007) indicates that the key is to find a series of cut offs that yield to a consistent ranking of the population given the observed outcome measures of deprivation. 1.3.2 Challenges in aggregation and identification of the poor Finding the spaces of needs, dimensions and cut offs is just one stage in poverty measurement. (Thorbecke, 2007, p. 7) makes the following point: Now let us assume that, notwithstanding all the difficulties discussed above, agreement has been reached on a list of attributes related to poverty and their threshold levels. How can such information be used to derive measures of multidimensional poverty and make poverty comparisons? Start with the simplest case, for example,that of an individual who is below each and every attribute threshold level. Such a person would be classified as unambiguously poor. Analogously, comparing two individual poverty profiles (A and B) where the attribute scores for all of the n dimen-sions in the profile of A are above that of the profile of B, it can be inferred unambiguously that A is better off in terms of well-being (less poor) than B. The question about how to aggregate the indicators and then how to split the population into two meainful groups (i.e. poor and not poor) has been present since the classic studies of poverty. From a theoretical perspective there have been three responses to this question. Townsend (1979) put forward a theory that stated that there must be a level of resources (the cause) from which deprivation raises substantially (consequence) (This is also known as the Townsend breaking point). Such a point should be the poverty line in that it leads to a meaninful split of the population, i.e. people whose standard of living is so low that they are effectively excluded from the patterns of living in society. Within this approach the aggregation consists in counting the number of deprivations and finding an optimal split based on its relationship with resources. P. Townsend &amp; Gordon (1993) argue that when using cross-sectional data, the best approach is the intersection -below certain level of resources and above certain level of derpivation- (See (Gordon, 2010)). This is the approach, for example, adopted to identify the poor in Mexico (CONEVAL, 2011a). One distinctive aspect of the Townsend breaking point is that the dimensions do not figure although the accounting of the diversity of attributes of deprivation remains. A second theory-driven approach consists in using the union approach (being deprived in either resources or basic needs). This approach is used as part of Boltvinik’s integrated method (Boltvinik &amp; Hernández-Láos, 2001). The IMMP aims to minimize the exclusion of the poor and the view that income, deprivation of needs and time deprivation are three attributes to characterise poverty. This is different from (Townsend, 1979)’s theory where needs deprivation is a manifestation of low command of resources (See (Gordon, 2010, p. @Cortes2014) for a discussion from the perspective of poverty dynamics and human rights). Unlike the first two approaches, the third main approach focuses more on the aggregation of the indicators than on how to set the poverty line (Foster, Greer, &amp; Thorbecke, 2010). The axiomatic approach was pionereed by Sen (1976) for income-based measures. Sen (1976) put emphasis on the importance of the aggregation stage in poverty measure as it had consenquences for the decomposition and analysis beyond the prevalence of poverty. Two axioms were critical for Sen (1976): monotonicity (poverty rise if income falls) and transfer (poverty rise if there is a transfer from the poor to the rich). Later Foster, Greer, &amp; Thorbecke (1984) extended Sen’s axioms and this formulation has been developed for multidimensional measures (Tsui, 2002, pp. Alkire2011a, Alkire2015). The AF index respect several desirable axioms such as monotonicity and other axioms such as symmetry, replication invariance, scale invariance, poverty focus and population subgroup decomposability (Alkire et al., 2015, see for a complete description). It results in an aggregation method that produces and index with properties that are true without proof. \\[ P_{AF}(X;z)= \\frac{1}{n}\\sum _{i=1}^{n}\\sum _{j=1}^{d}w_{j}g^{\\alpha }_{jk}(k);\\alpha\\geq0 \\] In their formulation \\(x\\) are achievements in the form of indicators or dimensions from a matrix \\(X\\) and \\(z\\) and the cut-offs for identifying the poor \\(x_{ij}&lt;z_j\\), \\(i\\), are individuals or households and \\(j\\) the dimensions. A person is deprived in the dimension (\\(g^{0}_{ij}=1\\)). Weights are included via \\(w_j\\), and \\(\\alpha=0\\) is the adjusted headcount ratio and \\(\\alpha=1\\) the adjusted poverty gap. One crucial aspect that is often and wrongly overlooked when considering the axiomatic approach is the difference between an aggregation method (formula) and a measurement methodology (series of steps). The Aliker-Foster (AF) family of measures impose a series of axioms so that the behaviour of a formula is predictable. The fact that is based on axioms does not make the measurement of poverty based on the AF method true or correct. The axiomatic approach, as any other measure, is based on the key assumption that the indicators (cut offs), weights and dimensions are a sensible account of poverty. Unlike the aggregation used in the counting approach, this aggregation method, explicitly takes into account relative welfare weights (S. Alkire &amp; Foster, 2011). The issue of weighting will be discussed in detail in Chapter 3 but as in the previous case, measurement theory offers a framework to deal with this issue under a falsifiable framework. One of the aims of this book is to clarify why the principles of measurement theory are a necessary condition for the AF to work, whereas the AF is not so for an index that fulfills the core principles of measurement theory. 1.4 The poor and the not poor: The poverty line From a conceptual basis, the poverty line could be defined as the minimum living standards acceptable in a society at a given point in time. The poverty line is often expresed in either monetary or non-monetary terms (i.e. deprivation weighted count). This is not a book on income poverty measurement but when using an indirect approach Van den Bosch (2001) identifies the following methods.2 Peter Townsend (1993) disusses the problems with this type of approach: Budget standards: The price of a specific basket of goods and services. This approach was used by Rowntree (1901) and recently most notably by Bradshaw (1993) and Bradshaw et al. (2008). Official standards: An arbitrary price or deprivation count is used by an statistical agency to split the population. It is often based on the minimum income support offered by the social security system. Food-ratio method: It assumes that living standards can be judge by comparing the proportion of income spend on necessities. Relative method: The income threshold is set to a certain percentage of the mean or median income. Some european countries use 60% of the median income, for example. The introduction of direct approaches to capture poverty has opened up a discussion about whether income and deprivation should be combined to identify the poor (Boltvinik, 1998, 2014; Boltvinik &amp; Hernández-Láos, 2001; Gordon, 2010). In the literature, this is known as the debate between union and intersection approaches. The union approach sees income and deprivation as two measures of the same phenomenon and therefore being below of a certain cut off in either of the two leads to poverty. In contrast, the intersection approach ackowledges that these should be used jointly to identify the poor, i.e. lacking both sufficient income and being multiply deprived is poverty. For Boltvinik &amp; Hernández-Láos (2001) the union minimizes the exclusion of the poor and the intersection maximizes its exclusion. So the first one overestimates poverty and the second is an underestimate. A third option is a partial union approach (S. Alkire &amp; Roche, 2011; Santos &amp; Villatoro, 2016). Income is first dichotomized using \\((x;z)\\) a cut off. Then, income is included in a score as another deprivation in the AF aggregation method. The partial inclusion is obtained by using differential weights and by setting a poverty line based on a percentage of the total possible deprivation score (Typically 25%). One practical argument in favour of the intersection approach is given from the perspective of poverty dynamics. When using cross-sectional data is not possible to assess the individual trend in deprivation followed a rise or fall of resources. That is, some households having just experienced a rise in income are expected to be less deprived in the incoming future. However, the snapshot of the cross-sectional survey would put them as poor under the union approach when in reality they are at risk or vulnerable to poverty (Katzman, 2000). Only with high-quality panel data would be possible to identify the truly poor based on the union approach (Gordon, 2006; Halleröd, 1995). When discussing poverty lines is important to ask what is the underlying theory behind the different approaches. Unfortunately, theories are rarely explicit and inprecise with regard the poverty line. There are three main approaches to set the poverty line: Townsend’s breaking point: One explicit framework is given by by Townsend (1979) and Peter Townsend (1993). One of the predictions of Townsend’s theory is that there is a negative relatioship between resources and deprivation. Yet, he proposed that such relationship is not linear and that multiple deprivation raises considerably below a certain level of resources. Normative approach based on social rights: Social rights are indivisible and interelated so being deprived of one right captures a situation of denial of basic human needs. CONEVAL (2011a) uses 1+ deprivation and an income below an income poverty line (drawn from a budget standards approach) to identify the poor . Normative approach integrated method: Boltvinik &amp; Hernández-Láos (2001) proposes that poverty has three dimensions: time, resources and UBN. People is regarded as poor when failing to meet one of the three dimensions. Arbitrary approach: Without any theoretical justification, some researchers set the poverty at some value below 50% of the total weighted sum of deprivations. For example, if there are 10 deprivation and the cut off is 30%, people with three or more deprivations are regarded as poor. These theoretical and methodological discrepancies lead to considerably differences in the extent of poverty. This is true even when the same indicators are utilized for two measures with different strategies to set the poverty line. This begs the question about how to tackle this challenge from an empirical perspective. This topic is covered in the next chapter of the book. 1.5 A brief on multidimensional poverty measurement The debates about the definition of poverty and the best way to measure it have, of course, been reflected in different approaches to capture this phenomenon. The diverse views about how to produce a poverty scale have shaped the history and types of poverty measures. Poverty research and measurement has more than 100 years of history. The ground-breaking and now classic studies of Rowntree (1901) Poverty: A study of Town Life', @Booth1903'sLife and Labour of the People in London’ and Townsend (1979)‘s `Poverty in the UK’ were possible after years of relentless research that aimed to understand the extent, distribution and nature of poverty. A common feature across these studies is that all used an ex ante developed questionnarie to capture poverty. This is in stark constrast with current practices in poverty research as poverty is seldom measured using explicitly developed questionnaries. There are important differences among these three monumental studies. Perhaps the most fundamental is that Townsend (1979)’s study was fully theory driven and used direct indicators of poverty (deprivation) and not income. The legacy of these studies has been such that the poverty research agenda dramatically expanded in the late XX and early XXI Centuries. Figure 1.1 syntetizes rather crudely the recent history of multidimensional poverty measurement. The multidimensional approach to poverty has its roots in both Europe and Latin America in 1960s and 1970s. Latin America was at the forefront in multidimensional poverty measurement thanks to the Unsatisfied Basic Needs (UBN) approach which used direct indicators to capture poverty [Altimir (1979);Beccaria1985;Boltvinik1992;Boltvinik2001}. The indicators have been mainly focused on housing and essential services given that the implementation of the UBN has been constrained by the avaiable data. Boltvinik &amp; Hernández-Láos (2001) adjusted variant of the UBN approach is perhaps the most theoretically comprenhensive and progressive in that includes time as a dimension. The UBN has become a widely known tradition in ppoverty measurement and has shaped contemporary measures such as the United Nations Development Programme (UNPD) multidimensional poverty index (MPI) and has greatly influenced official measures such as the Mexican measure CONEVAL (2011b). In the developed world, Townsend (1979)’s theory of deprivation not only served to produce the first survey questionnarie to measure multidimensional poverty but also proposed a direct and multidimensional measure of poverty. In the 1980s, the use of direct indicators to measure poverty was further suggested during the series of exchanges between Townsend (1985) and Sen (1983) and Sen (1985). This despite the focus of their argument was relative versus absolute poverty. This is not a book on monetary poverty but Sen’s axiomatic framework set the bases for the development of axioms for multidimensional measures Sen (1976) and Foster et al. (1984). In the late 1980s and 1990s Townsend’s influence on poverty measurement was boosted by Mack &amp; Lansley (1985)’s consensual approach that added the exploration of needs as a vital component to relative deprivation theory. Europe continued with the relative deprivation tradition but other parts of the world introduced monetary measurement of poverty (including Latin America), which became the mainstream approach in developing countries and led to the World Bank approach and the inevitable and rich debate about it (Pogge, 2005; Ravallion, 2010; Reddy &amp; Pogge, 2010). Figure 1.1: Some cool caption The XXI Century was witnessed the resurgense of multidimensional measurement. In Europe the Poverty and Social Exclusion series has continued Townsend’s tradition in the UK Townsend &amp; Gordon (2000), Pantazis, Gordon, &amp; Levitas (2006) and Mack &amp; Lansley (1985) have had a major influence in the measurement of poverty in Europe (Atkinson, Guio, &amp; Marlier, 2017; Whelan et al., 2006). In economics, the axiomatic approach has been finally extended for multidimensional measures (Foster et al., 2010; Tsui, 2002). These contributions and the capability theory as overarching framework has been very influential in multidimensional poverty measurement (S. Alkire &amp; Foster, 2011; Alkire et al., 2015; Kakwani &amp; Silber, 2008). The efforts to articulate human rights and multidimensional poverty measurement have helped to the progressive institutionalisation either by nations or by international institutions of official multidimensional measures (Boltvinik, 2014; CONEVAL, 2011a). The work Gordon et al. (2003) on child poverty pioneered the era of global harmonized multidimensional poverty measurement by drawing upon human rights and UBN. The United Nations Development Programme (UNDP) global acute poverty measure in collaboration with the Oxford Poverty and Human Development Initiative (OPHI) that recovers some key dimensions of the UBN approach and aggregates the indicators using the AF methodology (Alkire &amp; Santos, 2010; UNDP, 2014). The most recent regional example is the EUROSTAT deprivation index that draws on relative deprivation and the consensual method (Guio, Gordon, Marlier, Najera, &amp; Pomati, 2017; Guio et al., 2016). Only two official measure have been put to statistical scrutiny: EUROSTAT and CONEVAL. The EUROSTAT measure was produced by Guio, Gordon, &amp; Marlier (2012) and revalidated by Guio et al. (2017). It is based on the consensual approach and relative deprivaion theory and has been fully statistically validated. The reason why there are several arrows pointing at the CONEVAL measure (the official Mexican measure) is that this is arguably the first official multidimensional measure and it had a desirable process for its production. After a critical change of the Social Development Law in 2004, Mexico was obliged to measure poverty from a multidimensional perspective and in accordance with the constitution. This resulted in an international consultation process. Mora (2010) compiled the different contributions of the authors and CONEVAL (2011b) summarises the contributions of each author. The resulting measure is a hybrid measure that benefit from an international collaboration and it has been the first one to be fully statistically validated before its production. Townsend (1979) and the consensual approach were used to assess the tresholds for the nominal variables and provide a view on the socially perceived needs of the mexican population with focus on social rights. Then Gordon (2010) conducted a preliminar evaluation of the indicators of the measure and suggested an approach to identify the poor and the not poor. Then the AF method was used to estimate the depth and intensity measures for income and the deprivation score. Both EUROSTAT and CONEVAL have succeded in producing measures that are not only theoretically sound but are also empirically validated. This is, nonetheless, an exeption to the current practices in poverty research. The remainder of the book thus focuses on how to avoid producing magnifying-noise scales. The notation is fully introduced in the following sections but will introduce it also little by little to help not mathematical readers: specific outcomes/deprivation/achievements \\(\\mathbf{x}\\) and the full set of outcomes \\(\\mathbf{X}\\))↩ The calorie-based income poverty line is another popular method, which is based on the pressumption that the poor cannot meet their energy requirements -based on a reference basket-↩ "],
["poverty-and-measurement-theory-a-statistical-framework.html", "Chapter 2 Poverty and measurement theory: A statistical framework 2.1 Workflow in poverty measurement: A falsifiable framework 2.2 Identification of the sampling space 2.3 Selection of dimensions and indicators 2.4 Aggregation and weighting 2.5 Measurement theory as an statistical framework 2.6 Poverty and error in measurement 2.7 Measurement model for poverty 2.8 Blueprints and poverty measurement models", " Chapter 2 Poverty and measurement theory: A statistical framework Abstract This chapter outlines a statistical framework to tackle some of the challenges that involved in multidimensional poverty measurement. Measurement theory is posed as overaching framework for the assessment of some of the key assumptions made in the production of multidimensional poverty scales. The concepts of reliability, validity, measurement invariance and scaling are defined and put into the context of poverty research. Working examples of this concepts are then presented in the following chapters of the book. 2.1 Workflow in poverty measurement: A falsifiable framework Chapter 1 outlined some of the tasks involved in the development of multidimensional poverty indices and highlighted the assumptions underlying each stage: dimensions, indicators and thresholds or cut-point selection; weighting of dimensions and indicators; aggregation or production of a score and setting of a poverty line Alkire (2007), Thorbecke (2007) and Gordon &amp; Nandy (2012). Figure 2.1 ilustrates the strategy often followed by researchers to produce a poverty index. Data often precedes the measurement (data ara given). Therefore, researchers have little influence upon the data collection process and thus they are constrained and have to adapt the existent information on deprivation to their poverty definition -or in the worst cases, they have to adapt the definition ot the data-. A series of assumptions are raised with regard the number and type of dimensions, the indicators (including cut offs to identify deprivation) and weighting (see Chapter 1. Ideally, these assumptions should be assessed using a framework but in practice researchers avoid this stage or they conduct a series of ad hoc sensitivity analyses. The limitation is that these kind of analyses have no hypothesis and researchers are more likely to confirm their beliefs due to the lack of a clear testable strategy. Confirmation biases, therefore, are more likely to remain and basically researchers jump from their theoretical measure to the aggregation procedure. Figure 2.1: The workflow researchers often implement in multidimensional poverty measurement. Scientific measurement aims to incorporate a framework to falsify researcher’s assumptions. Figure 2.2 shows the strategy that researchers could employ to reduce confirmation biases and measurement error. A theory of the concept of poverty should guide the development of a survey questionnarie and data collection. Researchers then could further especify the structure of their poverty measure based on the theory-driven data generation process. Once the theoretical measure is defined, assumptions need to be identified and make explicit so that can be falsified using a sound statistical framework. The results can be used as an input to redefine the measure in an interative process. Once the measure is proven to be robust (using a definition of what a good measure is derived from a measurement framework), researchers could move onto aggregation and the identification of the poor and not poor groups. Figure 2.2: Ideal workflow in multidimensional poverty measurement. One possible and useful way to advance in the empirical assessment of poverty indices consist in detecting the assumptions involved in the production of an index and translate them into a falsifiable framework. That is, raising questions about all researcher’s assumptions and propose a method to answer whether researcher’s ideas hold given the data. The challenges involved in poverty research can be put in terms of sequential stages following list of problematic or contested issues in poverty measurement. All these issues are part of an effort to approximate the extent and identification of poverty and can be therefore written in terms of a statistical model: a simplified and imperfect description based on observed data. As McCullagh (2002) argues the idea of a statistical model conveys the recognition that the characterization of something is just an approximation. As such, a model to measure poverty is one possible option that can be assessed and improved. Before saying how a poverty measurement model can be examined and improved, is important to present an organised falsifiable framework of the key assumptions in poverty measurement. 2.2 Identification of the sampling space One of the often overlooked features in poverty measurement is that fact that researchers never work with the full set of information. The main reason is that such set is unknown or unavailable. Given a definition of poverty, there are different dimensions, indicators and parameters to produce a working model to approximate poverty. But all these option belong to a space with all possible options. This is a major aspect of poverty measurement and highlights the fact that there is an underlying assumption (based on theory or data) about what subsets would work better to measure poverty. Sampling space of all possible dimensions \\(\\mathscr J\\) Sampling space of all possible variables \\(\\mathscr X\\) Sampling space of all possible parameters (e.g. weights) \\(\\Theta\\) This is just a statistical model throughout which poverty is identified and can compactly be written as: \\[\\begin{equation} \\mathscr F = \\{\\mathscr X, F_{\\theta} : \\theta \\in \\Theta\\} \\end{equation}\\] where the variables \\(x_1,...,x_n\\) follow a certain distribution \\(F_{\\theta}\\), which is indexed by a parameter \\(\\theta\\) defined in the parameter space \\(\\Theta\\). \\(\\mathscr F\\) is a family of all probability distributions on \\(\\mathscr X\\), which is just the set of all possible observed data. 2.3 Selection of dimensions and indicators Poverty measurement therefore involves sampling from different spaces (array of options). For example, they put forward some dimensions, variables and weights. This is often the first challenge in poverty measurement where researchers select some dimensions \\(j\\) from the sample space \\(\\mathscr J\\) and some \\(x_{ij}\\) from the sample space \\(\\mathscr X\\), so that if the scale has 30 indicators \\(\\mathscr X={1,2,...,30}\\) where \\(x_{ij}=1\\) when deprived and \\(x_{ij}=0\\) (and \\(z&gt;0\\) for binary variables). For nominal variables, there is a space of possible cut offs \\(\\mathscr Z\\) too. Selecting dimensions, indicators and thresholds involves making the assumption that the sampling from the different spaces is the best possible one, i.e. the one that leads to a good poverty measure How does a researcher know whether its selection(sampling) is not wrong? This can be broken down into several questions: Is the subset of dimenions \\(j\\) from \\(\\mathscr J\\) an characterization of poverty? Is the subset of indicators given a cut off \\((X;z)\\) from \\(\\mathscr X\\) an characterization of the dimension \\(j\\) and poverty? The word adequate is a loose term as this point as we need a theory or standard to define it. This is covered in the next section but at this point, the focus is on the into traslating the challenges in poverty research into assumptions to build a statistical model (that later can be testable in some way). 2.4 Aggregation and weighting The first stage is focuses in indicator and dimension selection and as it will be discussed below, this is where a falsifiable framework is more useful but also absent in poverty research. One way to ilustrate it is with the AF method. The AF method will work fine as long as the components of the formula work fine but there is nothing within it that will ensure that this will be the case. Therefore, once the indicators have been selected, in a second state, researchers aggregate the variables using a linear model selecting some weights \\(w \\in \\mathscr W\\), where \\(w_{ij}=1\\) for non-differentially weighted measures and \\(w_{ij}\\ne 1\\). Does the weighting scheme lead to the same ranking of the population? 2.4.1 Splitting the population into meaningful groups Once the model of poverty has been completed \\(\\mathscr F\\) a score is produced for each person in the sample. That means that a threshold should be proposed to identify the poor population. 2.5 Measurement theory as an statistical framework 2.6 Poverty and error in measurement Poverty, as happens with many other constructs in social sciences, is an idea. It emerges from the theoretical presumption that within a population there is a group of people whose livings standard are below of what a society at a given point in time consider essential and customary to have a decent life. Poverty, in theory, impedes full participation in society, enhances the risk of die younger, interacts with many social risks, etc. But poverty is difficult to pinpoint because it is not directly observable and cannot be described with accuracy by a single variable. Instead, poverty research must rely on imperfect multivariate data to rank the population according to their living standards. To make things more difficult (see previous section), there are many ways in which poverty can be captured or described as researchers rely on different sets of outcomes (different samples from the same space) to characterise one’s living standards (diverse deprivation/needs/achievements). Poverty thus is a latent variable in that several outcomes are utilized to imperfectly describe it3. The problem of capturing poverty as a construct is just one of many cases in social sciences where researches face the difficulty of finding a theoretically constructed (unobservable) group (e.g. the depressed, high class, high-achievers in education, happy). How then can we detect a group of people belonging to a construct? Spearman (1904) put forward a capital idea: two or more outcomes is an indication that two things may have the same underlying cause. This perfectly fits the theory that deprivations are an observed outcome of poverty. A core assumption in poverty measurement is that a set (\\(X \\in \\mathscr X\\)) of indicators constitutes the series of relevant manifestations of the idea of poverty. This treatment of poverty as a construct is powerful in that is a theoretical underpinning and rationale for finding and ranking individual differences in living standards (below which one is poor). Deprivation indicators are intercorrelated because they share a common cause: poverty. Conceptually, this means that if the effect of poverty is eliminated the intercorrelation of outcome variables would be zero. Measurement theory postulates that the problem of the relevant set of outcome variables to measure poverty can be tackled from the perspective of the common factor model Thurstone (1947). Such a model formally postulates that the observed outcome is a function of one or more common factors and one unique factor. Each deprivation should vary due to two main sources (1) common variance and (2) unique variance. The first type of variance is the one accounted by for the latent factor- the variance shared with the other outcome measures. The second is the variance accounted by for other factors and by random error (unreliability, measurement error). 2.7 Measurement model for poverty The statistical model proposed in modern measurement theory would be a common factor model where each observed variable (\\(x_{ij}\\)) is a product of a latent dimension (\\(\\eta_j\\)) and the higher order factor (\\(\\zeta_h\\)) overall poverty. One of the interesting aspects of this formulation is that as not everything is due to the latent variable, error theory of a measurement model includes the method effect. \\[\\begin{equation} \\tag{2.1} x_{ij} = \\lambda_{ij} \\eta_j + \\varepsilon_ij \\end{equation}\\] \\[\\begin{equation} \\tag{2.2} \\eta_j = \\gamma_{j} \\zeta + \\xi \\end{equation}\\] The \\(\\lambda_{ij}\\) and \\(\\gamma_{j}\\) are known as factor loadings and they capture the realtionship between the latent variables and the outcome measures, and between the dimensions and the overall latent variable. Of course, it is possible to have more \\(\\zeta\\)’s but in poverty research the presumption seems to be that dimensions are nested into one overall latent construct which is poverty (some especial cases will be shown in Section XX where the higher-order model is a third-order factor): Poverty → Dimensions → Subdimensions → outcome measures. This model specification is one way to statistically capture the way in which multidimensional poverty measurement is conducted in the contemporary literature. Researchers propose a series of dimensions and classify their proposed indicators accordingly. The crucial aspect is that this is just an idea -informed from some theory or data- about how poverty can be better captured under a multdimensional definition. That is, the proposal is just an invalidated model that demands scrutiny. When put in terms of equation (2.1) and equation (2.2) researchers move from theoretical especulation toward empirical falsification. 2.8 Blueprints and poverty measurement models Models are just blueprints that summarise an explanation of how things can be constructed. Putting theoretical proposals to measure poverty in terms of a diagram helps a lot to visualise and contrast the diverse proposals to measure poverty. The advantaje of measurement theory is that everything can be put in terms of a blueprint. Multidimensional models can be easily presented as a diagram, which is just a graphical representation of a model. Another way to think about this, is to see the different blueprints in poverty measurement are just the different ways in which researcher sample from \\(\\mathscr J\\) and \\(\\mathscr X\\). They lead to diverse structures, i.e. models. Equation (2.1) simply tells a model in which one latent construct produces the observed (\\(x_{ij}\\)) outcomes. This model is say to be unidimensional as there is only one factor \\(\\eta_j\\) causing the observed indicators. One could think of this model as the null model in poverty measurement where the indicators cannot be clearly clustered into dimensions. This does not mean that the indicators do not measure different aspects of poverty. It might be that the indicators indeed different aspects but that there are no clusters of indicators. In practice, there are few theoretical models proposing such thing. Empirically, however, it might be often a case given that in poverty research data collection rarely follows a theoretical proposal (see add chapter reference). Figure 2.3 translates (2.1) into a plot. Figure 2.3: This is a visual representation of a null unidimensional model. Figure 2.4 displays the higher order factor model using the three dimensions proposed by . This structure is roughly what poverty researchers have in mind when thinking about poverty in multidimensional terms. That is, that the indicators can be grouped into some dimensions. In this example, there are 9 outcome variables classified into three dimensions (\\(\\eta_j\\)) (Durables, Housing and Economi strain). Then the loadings (\\(\\lambda_{ij}\\)) denote the relationship of each outcome with the dimensions in question. Then the arrows from the high-order factor (overall poverty) to each dimension are the factor loadings (\\(\\gamma_{j}\\)) that capture the relationship between overall poverty and each dimension. Both (\\(\\lambda_{ij}\\)) and (\\(\\gamma_{j}\\)) are parameters of the model and denote the streghnt of the association between the latent variable and the outcome variables (See for an explanation of how this relates to the topic of reliability and differential weights). The diagram of @(fig:cfaguio) model is an example of a higher-order factor: A three-dimension model with a higher-order factor. The dimensions are durables, housing and economic strain. In this example, there are only three indicators for each dimension. In this model, none of the indicators loads into more than one dimension. Poverty measures often assume that a given indicator is an exclusive outcome of a certain dimension. Figure 2.4: This is a visual representation of A Guio et al. (2009)’s model. Second-order factor. Another way to think about the dimensions of poverty comes from Alkire &amp; Santos (2010), which is used to compute OPHI-UNDP’s Multidimensional Poverty Index (MPI). The structure proposes a similar structure to Guio et al. (2009)’s model: second-order structure. In this case, poverty is thought to have three substantive dimensions: Education, health and iving standards. These are different dimensions from those porposed by Guio et al. (2009) or by Townsend (1979) in figure 2.5. The diagram only specifies, at this point, the structure of the measure as the MPI does not have three indicators for each dimension but it helps to see the model the authors put forward to measure global acute poverty. Figure 2.5: This is a visual representation of Alkire &amp; Santos (2010)’s model. Second-order factor Townsend (1979)’s model is one of the first multidimensional models in the world. His proposal has more nested dimensions compared with figures 2.4 and 2.5. In this case, indicators are nested into eleven dimensions, which in turn can be grouped into two more dimensions. The resulting figure (2.6 is a third-order factor structure. Guio et al. (2017) propose a reduced version of this model which does not considers the 11 dimensions and indicators are classified according to material and social deprivation. The loadings of the indicators are ommited. \\(\\kappa_1\\) and \\(\\kappa_2\\) are the loadings of the higher order factor. These could be specified using (2.2) as reference- now \\(\\zeta\\) has k=2 factors. Figure 2.6: This is a visual representation of Townsend (1979)’s model. Third-order factor. Another way to think about this, is to see the different blueprints in poverty measurement are just the different ways in which researcher sample from \\(\\mathscr J\\) and \\(\\mathscr X\\). They lead to diverse structures, i.e. models. This reflection brings us back again to the question: how do we know whether the samples we take from \\(\\mathscr J\\) and \\(\\mathscr X\\) are an adequate representation of poverty in a given society at a given point in time? There is another framework that incorporates the idea of measurement error. Fuzzy sets theory uses the term vagueness to describe the intrinsic imperfection in poverty measurement (Martinetti, 2006)↩ "],
["measurement-theory-and-principles.html", "Chapter 3 Measurement theory and principles 3.1 Origins of measurement theory", " Chapter 3 Measurement theory and principles One central debate in poverty measurement is about the dimensions of poverty. The discussion revolves around questions such as: How many dimensions are? What are the contents (indicators) of these dimensions? How dimensions are associated and differentiated? Does these dimensions have equal importance? (see ). The previous section transtaled these questions into concrete challenges and advanced a general framework of measurement that explicitly ackowledges that all measurement practices involve different kinds of error. However, the framework is incomplete. It requires some governing principles that effectively put the workflow in poverty measurement in terms of a cogent falsifiable framework. One crucial question raised in section~ are: Is the subset of dimenions j from \\(\\mathscr J\\) an adequate characterization of poverty? Is the subset of indicators given a cut off \\((X;z)\\) from \\(\\mathscr X\\) an characterization of the dimension \\(j\\) and poverty? Does the weighting scheme lead to the same ranking of the population? 3.1 Origins of measurement theory Measurement consists in assigning a series of numbers to individuals in such a way that they represent quantitites of attributes (Nunnally &amp; Bernstein, 1994). Measurement theory is a framework that postulates that a series of outcome measures are manifestations of a latent trait in each individual. That is, a framework to turn a series of deprivation indicators into numbers so that they express the unobserved level of poverty of an individual. Hence, measurement theory aspires to provide a series of rules to distinghuish signal from noise so that our observed measures approximate the latent trait in question. The origins of measurement theory can be traced back to classical test theory (Lord, 1952; Novick, 1966). This theory postulated that a true score is a linear combination of an observed score and error. This idea has been taken forward by the latent variable approach through a series of breakthroughs in theory, conceptualisation via latent constructs and compuation (Cudeck &amp; MacCallum, 2012; Rusch, Lowry, Mair, &amp; Treiblmaier, 2017). The consolidation of this framework required parallel and temporarely disconnected contributions in factor analysis and item response theory (IRT). After Spearman (1904)’s seminal one factor model and Thurstone (1947) multiple factor contribution, as series of works in the 1960s proposed formulating factor analysis not in terms of a correlation matrix but in terms of a model Lazardfeld &amp; Henry (1968);Lawley &amp; Maxwell (1971). Later and independently, it seems, from factor analysis, in educational testing and psychometrics, a series of works from Stocking &amp; Lord (1983) and Bock &amp; Aitkin (1981) proposed item response theory (IRT) which proposes that indicators are measures of observed manifestations of an underlying trait (Reise, 2014). The birth of IRT was almost contemporary to Jöreskog (1970);Joreskog, Sorbom, &amp; Magidson (1979)’s contributions to confirmatory factor analysis and structural equation models. These factor models could be seen now as general case of IRT for categorical variables (Muthén, 1984). Modern measurement theory is a more unified framework that postulates that outcome measures are manifestations of a latent trait, that such manifestations could be clustered together into subdimensions, that measurement will always have error and that all these aspects should have an empirical counterpart to that can be tested (Cudeck &amp; MacCallum, 2012). Measurement theory since the seminal work of Spearman (1904) has continuosly developed a cogent framework that aims to produce measures that: 1) consistently offer the same ranking of a population and 2) a ranking actually represents an ordering of the population with respect the phenomenon we aspire to measure. The two aims gave birth to the concepts of reliability and validity. Furtheremore, these two have important implications in terms of weighting, comparability and identification of (unobservable) population groups. "],
["Chapter-3.html", "Chapter 4 Reliability in poverty measurement 4.1 Intuition to the concept of reliability 4.2 Reliability theory 4.3 Statistical measures of reliability 4.4 Item-level reliability and weighting 4.5 Estimation of Reliability", " Chapter 4 Reliability in poverty measurement Abstract This chapter introduces the theory and concept of reliability. An intuitive explanation is provided at the begining of the chapter to underline the implications of reliability for measurement. Then, a formal introduction to the theory of reliability is provided. Reliability can be estimated using different approaches, the chapter discusses its limitations. The second main section of the chapter ilustrates how reliability works and how can it be estimated using R and Mplus by using simulated data. Then a real-data example is used to show some of the tipical problems involved in the examination of reliability. 4.1 Intuition to the concept of reliability In what sense the concept of reliability relates to the idea of having a measure we can trust? Poverty analysts and policymakers require indices they beleive in to focus on more important issues like devloping and studying poverty erradication strategies. There is nothing worse in measurement that a scale that causes disbelief in that the debate concetrates upon how bad a measure is and not upon how good or bad a policy is. `Trust’ is build upon consistent and meaningful estimates. For example, imagine a case in which we could conduct two surveys to the same population. Ideally, we expect our classification of the poor population to remain unchanged from \\(t_1\\) to \\(t_2\\). This is telling us that our index is stable across samples. A noisy index, in contrast, would lead to unsatble orderings and it is impossible to distingush a singnal (the thing we are interested in) from noise (unnecessary and confusing variability). Consistency, however, is not simply having the same response patterns ceteris paribus across two samples but also by having systematic population orderings. Imagine a case in which one of the deprivation indicators is not a good measure of poverty, like having a folding bycicle. This variable will have a low correlation with the rest of the deprivation indicators. Spearman (1904) tells us to be suspicious about such kind of behaviour. Low correlation (or even worse negative correlation) could mean that the indicator in question in not a consequence by poverty (“Lack of command of resources over time” See Chapter 1). The consequence would be that we will end up with two different population rankings depending on whether we include folding bycicle in our index. How different? It will depend upon how poorly correlated the indicator in question is with the rest. Therefore, even with a very similar response pattern, our scale will be rather unstable to be trusted. Of course, if we know that the folding bycicle item is an unreasonable measure of poverty we would have drop it before the empirical analysis. However, in poverty measurement there are variables or thresholds of these variables that are quite contested. Now imagine a different escenario where we have only good outcome measures of poverty and, for some reason, a good variable like lacking drinking piped water inside the house is dropped from the index (assuming this is a developing country where this measure works!). If we dropped this indicator from our analysis we would lose valuable information. Because we will be missing good variables (either because are not available or we just miss it from theory) we would like a measure whose population ordering is not that sensible to information losses. That, indeed, is a measure we can trust in the sense that it will lead to consistent results. High reliability is a property that, for instance, protects an index against certain information losses, i.e. the higher the reliability, the lower the effect of missing variables. Yet, missing indicators could be damaging for policy reasons, of course. 4.2 Reliability theory Reliability is a key concept in measurement theory and can be simple defined as the homogeneity of an index (Revelle &amp; Zinbarg, 2009). An homogeneous index is a scale whose outcome indicators are manifestations of the same trait. In the literature several authors refer to reliability as internal consistenty of an index because this a consequense of homogeneity. In the example above having an indicator that is not a good measure of poverty means that the index is heterogeneous and therefore leads to inconistent population orderings. Thus at the core of the principle of reliability lies the idea of having a series of items that would have a predictable behaviour when aggregated, i.e. if an index is reliable we should expect to have very similar population rankings across samples or small variations of the same reliable index with more or less indicators. The theory of reliability is inextricably connected with the evolution of measurement theory. The theory of reliability can be traced back to classical test theory (CTT) but it has been under continous development by more recent breaktrhoughs in latent variable modelling. Reliability is rooted in the acknowlegement that all measures have an unkown mixture of signal and noise (error). For Spearman (1904) there should be a true score- that is just the combination of an observed score and error. As in classical or frequentists statistics, it is true in the sense of the expected score across many replications of the same experiment. Being \\(\\theta\\) the true score, in CTT reliability is expressed as: \\[\\begin{equation} \\tag{4.1} x_i = \\theta_i + \\varepsilon_i \\end{equation}\\] Equation (4.1) can be put in terms of variance decomposition. The variance of the observed score \\(\\sigma^{2}_{x}\\) is thus equal to the variance of the true score plus the variance of the error. The discrepancy between the true score and the observed score is an estimate of reliability: \\[\\begin{equation} \\tag{4.2} \\rho = \\frac{\\sigma^{2}_{\\theta}} {\\sigma^{2}_{x} + \\sigma^{2}_{e}} \\end{equation}\\] where \\(\\rho\\) is the total reliability and \\(\\sigma^{2}_{i}\\) is the subject’s variability and \\(\\sigma^{2}_{e}\\) is the measurement error. Because this is a simple proportion, the reliability estimate will be (almost) always between 0 and 14. The classical definition of reliability has been translated and adopted by the latent variable approach. This approach is not at all concerned with the true score but with the extent to which a measure reflects the construct. Here the factor loadings \\(\\lambda_i\\)’s are key in that they reflect the association between an outcome and the latent construct. Therefore, latent variable approach naturally accomodates the question about how good are the manifest variables. Furthermore, it can estimate both \\(\\sigma^{2}_{x}\\) and \\(\\sigma^{2}_{e}\\). Reliability can be expresed as: \\[\\begin{equation} \\tag{4.3} \\rho_{x_{i}\\theta} = \\frac{\\lambda^2_{i}} {\\sigma^{2}_{x}} \\end{equation}\\] 4.3 Statistical measures of reliability The are different ways to estimate the reliability of a scale, each one with its advantages and disadvanatages. The most widely use estimate of reliability is \\(\\alpha\\) or \\(\\lambda_3\\) (do not mistake with factor loadings) (Cronbach, 1951; Guttman, 1945). This estimate comes from CTT and draws upon Spearman (1904) approach to estimate the variance based on parallel tests: \\[\\begin{equation} \\tag{4.4} \\alpha = \\lambda_3 = \\frac{\\sigma^{2}_{x} - \\sum\\sigma^{2}_{xi}} {\\sigma^{2}_{x}} \\frac{n} {n-1} \\end{equation}\\] Cronbach’s \\(\\alpha\\) is, nonetheless, not a good estimate of reliability (Revelle &amp; Zinbarg, 2009; Zinbarg, Revelle, Yovel, &amp; Li, 2005). It only works fine under very restrictive assumptions. First, the association between each indicator and the latent variable is equal. For example, for a measure based on three outcome variables it would mean that: \\(\\lambda_1=\\lambda_2=\\lambda_3\\). Second, the outcome measure have equal error variances. These two assumption are unlikely to hold in practice. Another problem with \\(\\alpha\\) is that increasing the number of items and the average inter-item correlation will increase the reliability estimate. Table~ summarises the relation among the different reliability statistics by dimensionality. Given that \\(\\alpha\\) is based upon untenable assumptions, there have been several proposals to estimate reliability under more general conditions. Revelle (1979) proposes the statistic \\(\\beta\\). This coefficient considers the worse split in different halves, i.e. it minimizes the average covariance by taking into account the lowest interitem correlation (\\(\\bar{\\sigma_{ij}}\\)). It is thus a measure of the lowest possible reliability and therefore it will always be lower or equal to \\(\\alpha\\). It is estimated as follows: \\[\\begin{equation} \\tag{4.5} \\beta = \\frac{k^2 \\bar{\\sigma_{ij}}} {\\sigma^{2}_{x}} \\end{equation}\\] McDonald (1999) put forward two alternate measures of reliability: \\(\\omega\\) and \\(\\omega_h\\). The first statistic is also know as the measure that maximizes the estimation of reliability, i.e. the lowest upper bound (Zinbarg et al., 2005). Equation (4.6) shows the formula of \\(\\omega\\). This equation is a proportion of the variance of the latent variable that is accounted by the outcome measures. \\[\\begin{equation} \\tag{4.6} \\omega = \\frac{ \\sum\\limits_{j=1}^k \\bigg(\\sum\\limits_{i=1}^p \\lambda_{ij}\\bigg)^2 } {\\sum\\limits_{j=1}^k \\bigg(\\sum\\limits_{i=1}^p \\lambda_{ij}\\bigg)^2 + \\sum\\limits_{i=1}^p e_i} \\end{equation}\\] Equation (4.7) shows the formula to estimate \\(\\omega_h\\) which is also a proportion but in this case is the variance accounted by the higher order factor. Therefore, this is a more appropiate measure when having multidimensional scales. \\[\\begin{equation} \\tag{4.7} \\omega_h = \\frac{ \\bigg(\\sum\\limits_{i=1}^p \\lambda_{ij}\\bigg) ^2 } {\\sum\\limits_{j=1}^k \\bigg(\\sum\\limits_{i=1}^p \\lambda_{ij}\\bigg) ^2 + \\sum\\limits_{i=1}^p e_i} \\end{equation}\\] These different reliability statistics beg the following question: Which one should be used? There are two complementary ways to answer this question. First, these reliability statistics are based on a series of assumptions and thus its usage depends on the extent to which each one is adequate given the data and the research question. \\(\\alpha\\) is a very specific case whose assumptions will be rarely meet in practice. The recommendation is to avoid using \\(\\alpha\\) and focus on general cases such as \\(\\omega\\) and \\(\\omega_h\\). \\(\\omega\\) will work in almost any situation but when the measures are multidimensional. This does not mean that it would be incorrect to use it. In multidimensional settings, \\(\\omega_h\\) is just more adequate because it will tell the amount of variance accounted by for the higher order factor. Zinbarg et al. (2005) ran a Monte Carlo study to assess how does the different reliability statistics compare one another. They found the following (See Table 4.1): Table 4.1: Summary of the relations among \\(\\beta\\), \\(\\alpha\\), \\(\\omega\\) and reliability depending on index dimensionality. Taken from (Zinbarg et al., 2005, p. 128) Dimensionality Expected behaviour Multidimensional \\(\\beta&lt;\\alpha&lt;\\omega\\leq\\rho\\) \\(\\omega_h&lt;\\omega\\leq\\rho\\) Unidimensional \\(\\beta&lt;\\alpha&lt;\\omega_h=\\omega\\leq\\rho\\) The second way to answer the question has to do with the conclusions one could make from the estimation of these measures. If the assumptions are violated our conclusions would be very likely incorrect and misleading. Assuming the correct statistic is selected, the question is: How low is too low to be unacceptable? One of the consequences of reliability is that it leads to an accurate ranking or ordering of the population in question, i.e. from the lowest standard of living to the highest. Nájera (2018) run a Monte Carlo study to assess the relationship between reliability and population classification. Hence, this study poses the question about the level of reliability that guarantees a low amount of error. The result was that there is a clear relationship between reliability and population classification. The summary of the findings of Nájera (2018) are shown in Table 4.2. The simulation considered three possible dimensional structures: unidimensional, weak and strong multidimensional measures. Weak multidimensionality was defined as the case where the dimensions have relatively low loadings to the higher-order factor. Table 4.2: Summary of the relations among \\(\\beta\\), \\(\\alpha\\), \\(\\omega\\) and entropy depending on index dimensionality. Summarised from Nájera (2018). In this case, the unidimensional model seem to meet \\(\\tau\\) equivalence, i.e. equal loadings. Reliabiity statistic Leads to lassification error (%) Entropy value \\(\\alpha&gt;.8\\) \\(\\approx\\) \\(&lt;5\\%\\) \\(&gt;.8\\) \\(\\omega&gt;.8\\) \\(\\approx\\) \\(&lt;5\\%\\) \\(&gt;.8\\) \\(\\omega&gt;.85\\) \\(\\approx\\) \\(&lt;5\\%\\) \\(&gt;.8\\) \\(\\omega_h&gt;.65\\) \\(\\approx\\) \\(&lt;5\\%\\) \\(&gt;.8\\) \\(\\omega&gt;.85\\) \\(\\approx\\) \\(&lt;5\\%\\) \\(&gt;.8\\) \\(\\omega_h&gt;.70\\) \\(\\approx\\) \\(&lt;5\\%\\) \\(&gt;.8\\) 4.4 Item-level reliability and weighting Classical test theory was concerned with overall reliability. Item response theory (IRT) move from the idea of a true score and look at the relationship of the indicators with an underlying trait (e.g. intelegence, depression, poverty) (Harris, 1989). IRT is a theory about the type of relationship that an indicator has with a latent variable. The simplest IRT specification proposes that a measure is undimensional (i.e. the variance of the indicators is accounted by for one trait) and that each item relates to different degrees of difficulty or severity of the construct. This is called a one-parameter IRT model. A more general IRT model also proposes that some indicators are better than others to differenciate the population. That is, that some deprivation indicators are associated with a higher likelihood of belonging to the poor group. This more general aspect is added via a second parameter called discrimination and leads to a two-parameter IRT model. This kind of model has been used by Guio et al. (2016) and Guio et al. (2017) for example. \\[\\begin{equation} \\tag{4.8} P_i\\theta = \\frac{1} {1+e^{-1.7a_i(\\theta-b_i)}} \\end{equation}\\] Equation (4.8), translated to poverty measurement, states that the probability of chosing a someone that is deprived in the indicator \\(i\\) is given by the discrimination (a) and the severity(b) of the item. Muthén (2013) show how this models relates to a unidimensional factor model, equations 21 and 22. In a factor model (b) is just a threshold and (a) the factor loadings (\\(\\lambda_{i}\\)). Therefore, the stronger the loadings, the higher its discrimination power, where \\(\\psi\\) is the variance of the latent variable. \\[\\begin{equation} \\tag{4.9} a_{i}=\\lambda_{i}\\sqrt{\\psi} \\end{equation}\\] The original IRT models assumed unidimensionality. However, this is no longer the case as it is possible to estimate multidimensional IRT model (Reckase, 2009). However, Gibbons, Immekus, Bock, &amp; Gibbons (2007) have shown that the presence of a higher-order factor produces little bias in the estimates when having more dimensions. In theory, all multidimensional poverty models make such an assumption. In any case, the concepts remain the same and a multidimensional IRT model can be simply connected with multidimensional confirmatory factor model. Statistics such as \\(\\beta\\), \\(\\alpha\\), \\(\\omega\\) provide an summary of the overall reliability. The computation of \\(\\omega\\) heavily relies on the factor loadings. The lower the factor loadings the higher the error and the lower the overall reliability. Similarly, low \\(\\lambda_{i}\\) can be traslated as low item-level reliability values. The question is thus how low mean unreliable. Guio et al. (2016) use the rule of \\(&lt;.4\\) standarised loadings as a measure of item-unreliability. Nájera (2018) shows that indeed those values are more likely to result in overall unreliability and high population classification error. One of the most contested issues in poverty measurement revolves around weighting (Decancq &amp; Lugo, 2013). Measurement theory proposes that reliability lead to a self-weighting measure in that it guarantees good population classification (Streiner et al., 2015). Discrimination parameters have a crucial role upon population classification and item weighting. The square of the factor loadings equals the amount of variance in the indicator explained by the common factor (i.e. communality). Because the factor loadings capture the relationship of each indicator with the latent variable, they can be seen as the optimal weights of the model given the data. Therefore, a test of equality of loadings within dimensional can be used to assess whether using such kind of weighting is reasonable or not. Nájera (2018) shows that very high reliability leads to a self-weighting index in that the population ranking is less sensible to the items used in a scale. Therefore, discussing the use of differential weights versus non-differential weights misses the point. The critical point is that differential weights, in that they are unkown, will always introduce more noise to the classification of the population. Whereas reliability is a necessary condition for good population orderings, weighting it is not so. One of the key axioms in poverty research is the monotonicity axiom. It states that poverty ceteris paribus should decrease after an improvement in one’s achievements (Alkire et al., 2015; Sen, 1976). Measurement theory states something very similar in that low loadings reflect the fact that changes in the latent variable do not lead to changes in observed deprivation. Nájera (n.d.) ran a Monte Carlo experiment the particularities of this behaviour. He finds that item-level unreliability leads to a violation of the monotonicity axiom. His conclusion is that indicators that have weak discrimination \\(\\lambda_ij&lt;.4\\) (standardised loadings) violate weak monotonicity and in some circumntances could violate strong monotonicity. Therefore, such indicators are more noise than signal to poverty measures. 4.5 Estimation of Reliability 4.5.1 Overall reliability To introduce the idea of reliability we will use the data set “Rel_MD_data_1_1.dat”. This is simulated data of a higher-order multidimensional measure of poverty (\\(n=5000\\)). The measure has nine indiactors in total distributed evenly in three dimensions. library(plyr) Rel_MD_1&lt;-read.table(&quot;Rel_MD_data_1_1.dat&quot;) Rel_MD_1$ds&lt;-rowSums(Rel_MD_1[,c(1:9)]) colnames(Rel_MD_1)&lt;-c(&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;,&quot;x4&quot;,&quot;x5&quot;,&quot;x6&quot;, &quot;x7&quot;,&quot;x8&quot;,&quot;x9&quot;,&quot;x10&quot;,&quot;x11&quot;, &quot;resources&quot;,&quot;educ_yr&quot;,&quot;occupation&quot;, &quot;class&quot;,&quot;hh_members&quot;,&quot;ds&quot;) Rel_MD_1[1:10,1:11] ## x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 ## 1 1 1 1 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 1 0 0 0 0 0 0 0 ## 4 1 1 0 0 0 0 1 0 0 0 0 ## 5 1 0 0 0 0 0 0 0 0 1 1 ## 6 1 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 1 0 1 0 0 0 0 0 ## 8 0 0 0 1 0 0 0 0 0 1 1 ## 9 1 0 0 1 1 1 1 1 1 0 0 ## 10 0 0 0 0 0 0 0 0 0 1 0 We do not know yet if our selected deprivation indicators lead to a reliable score. However, we can inspect its distribution by plotting it (Figure 4.1) as follows: require(ggplot2) ggplot(Rel_MD_1, aes(ds)) + geom_histogram() + theme_bw() + labs(x = &quot;Deprivation score&quot;) + scale_x_continuous(breaks = seq(0, 9, by = 1)) Figure 4.1: This is the histogram of the deprivation score. It shows the number of people by the equally weighted deprivation count. Now we can check the proportion of people deprived of each indicator as follows: dep_prop&lt;-unlist(lapply(Rel_MD_1, function(x) mean(x))) dep_prop&lt;-round(dep_prop[1:9]*100,0) dep_prop ## x1 x2 x3 x4 x5 x6 x7 x8 x9 ## 50 29 16 49 29 16 45 26 16 4.5.2 Exploratory (non-model based) estimation of overall reliability ow that we have familiarised with the data ourselves we can proceed to check the reliability of this scale. Reliability concerns with the homogeneity of a scale and its capacity to produce consistent rankings of a population. We will start by estimating the overall reliability of our scale using the psych package (Revelle, 2014). This is a comprenhensive R-package to estimate different reliability statistics (\\(\\alpha\\), \\(\\beta\\), \\(\\omega\\) and \\(\\omega_h\\)) under different changing conditions. The psych package can be used for exploratory and confirmatory settings for both unidimensional and multidimensional measures. This book focuses on confirmatory measurement models and to introduce the estimation of overall reliability we will rely on the simpliest way to estimate the homogeneity of a scale using the simulated data set. This will be further developed and the next section shows how psych interacts with another R-package lavaan to estimate \\(\\omega\\) and \\(\\omega_h\\) from a confirmatory factor model (Rosseel, 2012). The pysch package permits estimtating \\(\\alpha\\) and \\(\\omega\\) using the same function (omega). The package has several options but we know that there are three dimensions and one higher order factor and these values match the defaults of the omega function. It is important to bear in mind that in this simple case the value of \\(\\omega\\) is approximated with an Exploratory Factor Analysis (EFA). Below is shown how to do it with a confirmatory model. After aplying the omega() to our nine indicators, there will be different objects that store information with the results of the analysis. We will focus on the overall estimate of \\(\\alpha\\) and \\(\\omega\\) as here we are interested in knowing the homogeneity of our scale. We can appreciate below that both values are high (\\(\\geq.8\\)) (See for an explanation) and suggest that the scale is highly reliable. In this case, \\(\\alpha&lt;\\omega\\) indicating that this scale violates \\(\\tau\\) equivalence (equality of loadings). # install.packages(&quot;psych&quot;) require(psych) omega_exp1&lt;-omega(Rel_MD_1[,c(3:9)]) rel_uni_exp&lt;-data.frame(omega_exp1=omega_exp1$omega.tot, alpha=omega_exp1$alpha) rel_uni_exp ## omega_exp1 alpha ## 1 0.8599319 0.8127129 Both \\(\\alpha\\) and \\(\\omega\\) are easily estimated with the psych package. However, the previous example was pretty straightforward in that all the indicators are well-behaved. Thus to gain a deeper understanding of reliability and populatioin classification we will check what happens when one has indicators that reduce reliability. This can be done by adding noise to our measure. We will generate two uncorrelated indicators and substitute x10 and x11 for the indicators x1 and x2. Once the we have introduce some noise to our measure we will estimate a new deprivation score using the two new indicators and dropping x1 and x2. The result is shown below. Then we can apply omega() to the new matrix that includes V1 and v1 and excludes x1 and x2. Reliability has drop slitghly but enough to raise concerns as both \\(\\omega\\) and \\(\\alpha\\) are below the rules of thumb drawn from a Monte Carlo experiment. #Computing deprivation score with uncorrelated items Rel_MD_1$ds_ur&lt;-rowSums(Rel_MD_1[,c(3:11)]) Rel_MD_1[1:10,c(16,17)] ## hh_members ds ## 1 2 4 ## 2 1 0 ## 3 1 1 ## 4 2 3 ## 5 2 1 ## 6 1 1 ## 7 1 2 ## 8 2 1 ## 9 2 7 ## 10 2 0 #Now reliability drops omega_unr_exp&lt;-omega(Rel_MD_1[,c(3:11)]) unrel_uni_exp&lt;-data.frame(omega_exp=omega_unr_exp$omega.tot, alpha=omega_unr_exp$alpha) unrel_uni_exp ## omega_exp alpha ## 1 0.799405 0.738685 What is the impact of introducing the two uncorrelated indicators? From theory is known that losses in reliability affect the consistency of population classification. We can check if this theory holds by looking at the correlation of different rankings that are produce from different measures. For this experiment, first, we will estimate the omega values using different combinations of items (in all case we have the seven items from the reliable measure x1-x9). omega_exp2&lt;-omega(Rel_MD_1[,c(3:9)]) omega_exp3&lt;-omega(Rel_MD_1[,c(1,2,4,5,7,8)]) omega_exp4&lt;-omega(Rel_MD_1[,c(2,3,5,6,8,9)]) omega_exp5&lt;-omega(Rel_MD_1[,c(1,3,4,6,7,9)]) omegas_exp&lt;-data.frame(omega_exp1=omega_exp1$omega.tot, omega_exp2=omega_exp2$omega.tot, omega_exp3=omega_exp3$omega.tot, omega_exp4=omega_exp4$omega.tot, omega_exp5=omega_exp5$omega.tot, omega_unrel=omega_unr_exp$omega.tot) We then can compare the omega values of each measure. The theory holds for this example. We see that the lowest reliability scale is the one that incorporates V1 and V2. The measures with only seven items have higher reliability. This is a very important lesson as poverty researchers sometimes keep unreliable indicators in their scales and the consequence will be a heavy loss in reliability. t(omegas_exp) ## [,1] ## omega_exp1 0.8599319 ## omega_exp2 0.8599319 ## omega_exp3 0.8779016 ## omega_exp4 0.8543497 ## omega_exp5 0.8441886 ## omega_unrel 0.7994050 The second prediction of reliability theory is that the population orderings are consistent for high reliability values. One way to check this is by estimating the correlation among the different deprivation scores. Again, the theory holds for this simple exercise, the measure with higher \\(\\omega\\) are highly correlated. The correlation of the unreliable measure seems still high, however, when \\(\\omega&lt;.8\\) we could expect to see a classification error \\(&gt;5\\%\\) which might be very worring when put into perspective. If the poverty rate is \\(20\\%\\) and the classification error is \\(5\\%\\) it would mean that potentially a \\(25\\%\\) of the poor are mistakenly classified . Rel_MD_1$ds_r2&lt;-rowSums(Rel_MD_1[,c(3:9)]) Rel_MD_1$ds_r3&lt;-rowSums(Rel_MD_1[,c(1,2,4,5,7,8)]) Rel_MD_1$ds_r4&lt;-rowSums(Rel_MD_1[,c(2,3,5,6,8,9)]) Rel_MD_1$ds_r5&lt;-rowSums(Rel_MD_1[,c(1,3,4,6,7,9)]) ds.m&lt;-(Rel_MD_1[,c(16:21)]) ds.cor&lt;-cor(ds.m) ds.cor ## hh_members ds ds_ur ds_r2 ds_r3 ds_r4 ## hh_members 1.0000000 0.4330385 0.3945498 0.4077791 0.4422686 0.3777748 ## ds 0.4330385 1.0000000 0.9272651 0.9684719 0.9764524 0.9523882 ## ds_ur 0.3945498 0.9272651 1.0000000 0.9520979 0.8899044 0.8941369 ## ds_r2 0.4077791 0.9684719 0.9520979 1.0000000 0.9284786 0.9370200 ## ds_r3 0.4422686 0.9764524 0.8899044 0.9284786 1.0000000 0.8875033 ## ds_r4 0.3777748 0.9523882 0.8941369 0.9370200 0.8875033 1.0000000 4.5.3 Model-based estimation of overall reliability The ideal workflow in poverty measurement leads to a specification of a model. Diffrent models suggest that poverty is multidimensional and hierarchical (See Section 1.2). Therefore, the interest is in both estimates of reliability: overall and hierarchical omega (\\(\\omega\\) and \\(\\omega_h\\)). Both can be estimated from an EFA using the R-package “psych”. However, this book is an attempt to encourage poverty researchers to walk toward the production and assessment of theoretical models. To estimate reliability for a pre-especified model, it is necessary to use Confirmatory Factor Analysis (CFA). Given a pattern loading specification, a CFA will estimate the different parameters of the model. Section 4.3 showed the formulas to estimate both \\(\\omega\\) and \\(\\omega_h\\). Item-factor loadings and the residuals of the model are the key parameters for the estimation of both reliability statistics (See equation (4.6)). In the following we will show how in both Mplus and R is possible to estimate \\(\\omega\\) and \\(\\omega_h\\). We will start with R and for this purpose we need the “lavaan” package (Rosseel, 2012). This package comprises a series of functions to estimate different kinds of latent variable models such as measurement and analytic models like Structural Equation Models (SEM). Once the CFA model is fitted with the R-package lavaan, the function omegaFromSem() of the psych R-package can be used to estimate \\(\\omega\\) and \\(\\omega_h\\). However, we will show how this can be done by hand to gain insight of the differences between the two reliability statistics and to operationalise the process using the Mplus estimates. #Omega from Sem library(lavaan) # We first specify the model MD_model &lt;- &#39; h =~ +x1+x2+x3+x4+x5+x6+x7+x8+x9 F1=~ + x7 + x8 + x9 F2=~ + x4 + x5 + x6 F3=~ + x1 + x2 + x3 h ~~ 0*F1 h ~~ 0*F2 h ~~ 0*F3 F1 ~~ 0*F2 F2 ~~ 0*F3 F1 ~~ 0*F3 &#39; To fit the CFA model we will use sem function which has been harmonised with the functions cfa and lavaan. The function requires specifying the measurement model (MD_model), the data, the kind of variables we have (in this case categorical) and we will request standarised loadings with std.lv=TRUE. fit &lt;- sem(MD_model, data = Rel_MD_1, ordered=c(&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;,&quot;x4&quot;,&quot;x5&quot;, &quot;x6&quot;,&quot;x7&quot;,&quot;x8&quot;,&quot;x9&quot;), std.lv=TRUE) # The command below is to check the output (We will check this in the #next section and validity chapter) #summary(fit, fit.measures=TRUE, rsquare=TRUE, standardized=TRUE) Both \\(\\omega\\) and \\(\\omega_h\\) can be manually calculated. There are two main paramaters one needs for their computation: factor loadings from the indicators to the overall factor (\\(\\lambda_h\\)), to each dimension (\\(\\lambda_j\\)) and the error. This can be easily extracted from the fit object as follows: lambdas&lt;-as.data.frame(fit@Model@GLIST$lambda) error&lt;-colSums(fit@Model@GLIST$theta) The then the square of the sum of the loadings (\\(\\lambda_h\\)) and (\\(\\lambda_j\\)) is taken as well as the sum of the error. The we can compute both \\(\\omega\\) and \\(\\omega_h\\) using equation (4.6) and (4.7). Slambda_2&lt;-sum(lambdas[1])^2 + sum(lambdas[2])^2 + sum(lambdas[3])^2 + sum(lambdas[4])^2 error &lt;- sum(error) omega_t &lt;- Slambda_2 / (Slambda_2+error) omega_h &lt;- sum(lambdas[1])^2 / (Slambda_2+error) omegamanual&lt;-c(omega_h=omega_h,omega_t=omega_t) omegamanual ## omega_h omega_t ## 0.8445022 0.9707344 Fortunately, there is an R function from the “psych” package that does this for us. Once the model has been fitted, we apply the function omegaFromSem() to request the estimates and store the estimates of both \\(\\omega\\) and \\(\\omega_h\\) in the omegasem object. The results indicate high overall reliability and high reliability after considering the multidimensional features of the scale. omegasem&lt;-omegaFromSem(fit) omegasem&lt;-c(omega_h=omegasem$omega, omega_t=omegasem$omega.tot) omegasem ## omega_h omega_t ## 0.8446990 0.9707276 The R package “mplusAutomation” is an excellent alternative to automate Mplus from R (Hallquist &amp; Wiley, 2018). We can create within R an Mplus object as follows using the function mplusObject(). The syntax is the standard Mplus syntax to fit a model. As with lavaan we will fit a bi-factor model. We will store the syntax in the object test. test &lt;- mplusObject( TITLE = &quot;Bi-factor model CFA;&quot;, VARIABLE = &quot; NAMES = x1-x9 resources educ_yr occupation class; CATEGORICAL = x1-x9; USEVARIABLES = x1-x9;&quot;, ANALYSIS = &quot;ESTIMATOR = wlsmv; PROCESS = 4;&quot;, MODEL = &quot;f1 by x1-x3; f2 by x4-x6; f3 by x7-x9; h by x1 x2 x3 x4 x4 x5 x6 x7 x8 x9; F1 with F2@0; F2 with F3@0; F3 with F1@0; h with f1@0; h with f2@0; h with f3@0;&quot;, OUTPUT = &quot;std stdyx;&quot;) To write the test object as an \"*.inp\" Mplus syntax file, we will use the function mplusModeler(). This function permits estimating the model directly using the option run. res &lt;- mplusModeler(test, modelout = &quot;rel_CFA_2.inp&quot;, writeData = &quot;never&quot;, hashfilename = FALSE, dataout=&quot;Rel_MD_data_1_1.dat&quot;, run = 1L) ## ## Running model: rel_CFA_2.inp ## System command: C:\\WINDOWS\\system32\\cmd.exe /c cd &quot;.&quot; &amp;&amp; &quot;Mplus&quot; &quot;rel_CFA_2.inp&quot; ## Reading model: rel_CFA_2.out Once the model has been run, we can import the output using the function readModels(). We will explore the full output in the next chapter as for now we will focus in the estimation of \\(\\omega\\) and \\(\\omega_h\\). The factor loadings of the Bi-factor model are stored in a list (parameters). We request the standarised estimates as we did with lavaan. We also can request the error from the `r2 object in the parameters list. Once we have the parameters we need we can proceed as above to estimate the reliability statistics. We see that we could replicate the results from lavaan. REL_CFA_2&lt;-readModels(filefilter =&quot;rel_CFA_2&quot;) ## Reading model: C:/Proyectos Investigacion/PM Book/rel_cfa_2.out lambdas&lt;-REL_CFA_2$parameters$std.standardized[1:18,1:3] error&lt;-REL_CFA_2$parameters$r2[6] lambda_2&lt;-sum(lambdas[10:18,3])^2 + sum(lambdas[1:3,3])^2 + sum(lambdas[4:6,3])^2 + sum(lambdas[7:9,3])^2 error &lt;- sum(error) omega_t &lt;- lambda_2 / (lambda_2+error) omega_h &lt;- sum(lambdas[10:18,3])^2 / (lambda_2+error) omega_t ## [1] 0.9707333 omega_h ## [1] 0.8445348 4.5.4 Overall reliability and population orderings One of the predictions of measurement theory is that reliability leads to consistent population orderings, i.e. poor people will have high deprivation scores and not poor people will have low deprivation scores (see Table (#tab:relentropy)). We ilustrated this point using the correlation between the different deprivation scores corresponding to diverse levels of overall reliabilities. We can follow up that example by looking at the values of the latent variable for the multidimensional reliable measure (Rel_MD_1). After fitting the CFA model we just can simply use the function predict() to obtain the Maximum Likelihood estimates of the latent variable. Then we can merge these values with our data set. The prediction will generate four estimates for the latent variables. The overall factor (h) and the values for the three dimensions. factor_scores&lt;-predict(fit) Rel_MD_1&lt;-cbind(Rel_MD_1,factor_scores) head(Rel_MD_1[,c(21:24)]) ## ds_r4 ds_r5 h F1 ## 1 2 3 0.4475885 -0.81293477 ## 2 0 0 -0.6781638 -0.08627376 ## 3 0 1 -0.2440620 -0.28822588 ## 4 1 2 0.2851351 0.13806885 ## 5 0 1 -0.2207057 -0.30252106 ## 6 0 1 -0.2207057 -0.30252106 To contrast the values of the reliable multidimensional measure with the values of an slitghly less reliable measure we will fit a new model. As in the previous example (Section @ref(#Chapter-3-expoverel)), we will replace the first two indicators x1 and x2 by x10 and x11. Both load into the first factor (f1). The estimates are stored in a differen object (fit_ur) and estimate the factor scores using the predict() function. Finally we inspect the values. ## We first specify the model MD_model &lt;- &#39; h =~ +x10+x11+x3+x4+x5+x6+x7+x8+x9 F1=~ + x7 + x8 + x9 F2=~ + x4 + x5 + x6 F3=~ + x10 + x11 + x3 h ~~ 0*F1 h ~~ 0*F2 h ~~ 0*F3 F1 ~~ 0*F2 F2 ~~ 0*F3 F1 ~~ 0*F3 &#39; fit_ur &lt;- sem(MD_model, data = Rel_MD_1, ordered=c(&quot;x10&quot;,&quot;x11&quot;,&quot;x3&quot;,&quot;x4&quot;,&quot;x5&quot;,&quot;x6&quot;,&quot;x7&quot;,&quot;x8&quot;,&quot;x9&quot;), std.lv=TRUE) factor_scores_ur&lt;-predict(fit_ur) colnames(factor_scores_ur)[1:4]&lt;-c(&quot;hur&quot;,&quot;F1ur&quot;,&quot;F2ur&quot;,&quot;F3ur&quot;) Rel_MD_1&lt;-cbind(Rel_MD_1,factor_scores_ur) head(Rel_MD_1[,c(25:28)]) ## F2 F3 hur F1ur ## 1 -0.1296229 1.2478407 0.38436469 -0.7563179 ## 2 -0.1536510 -0.1720003 -0.59656764 -0.1126966 ## 3 0.5290560 -0.3986557 -0.10512455 -0.3763817 ## 4 -0.7559100 0.7442695 -0.05228111 0.4723843 ## 5 -0.3910445 0.4514504 -0.50856087 -0.1482380 ## 6 -0.3910445 0.4514504 -0.59656764 -0.1126966 To assess the consistency of both multidimensional scales we will plot the latent factor values by the deprivation score. Figure~4.2 shows that the factor scores are very similar within each deprivation group. For each deprivation score we find very different factor scores, indicating that the deprivation scores is a good measure to rank and split the population according to the severity of deprivation. In contrast, figure~@ref{(fig:fsdesunrel) show that although there is relationship between the deprivation score and factor scores, this relationship is more noisy. Not only there is much more variability within each deprivation group but also there is some overlap. That means that if we use some cutoff to split the poor from the not poor based on a deprivation score, we will be more likely to confound both groups. In this case, the mixing of groups is not that dramatic as the scale is still somewhat reliable, but it could be very noisy for less reliable scales. require(ggplot2) g &lt;- ggplot(Rel_MD_1, aes(as.factor(ds), h)) g + geom_boxplot(varwidth=T) + labs(x=&quot;Deprivation score. Reliable&quot;, y=&quot;Factor score (Latent variable)&quot;) + theme_bw() Figure 4.2: Relationship between the deprivation score (x1-x9) and the latent variable score. We appreciate the narrowness of the boxplots, indicating good group separation. g &lt;- ggplot(Rel_MD_1, aes(as.factor(ds_ur), hur)) g + geom_boxplot(varwidth=T) + labs(x=&quot;Deprivation score. Unreliable&quot;, y=&quot;Factor score (Latent variable)&quot;) + theme_bw() Figure 4.3: Relationship between the deprivation score (x10, x11 and x3-x9) and the latent variable score. There is more variability in this case indicating poor group separation. If the scale is badly constucted reliability could be negative using some statistics like \\(\\alpha\\)↩ "],
["references.html", "Chapter 5 References", " Chapter 5 References "]
]
