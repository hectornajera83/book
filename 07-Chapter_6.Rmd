
# Scale equating and linking

**Abstract**

This chapter focuses on a framework to make scales comparable across groups, countries and time points. It describes the theory of scale equating which proposes that two or more indices, with some indicators in common but not necessarily the same, can be made comparable via modelling. The chapter focuses in one form of equating: Item Response Theory equating because it is widely used, and it fits more naturally with the whole rationale of the book. Several examples are used to illustrate the main concepts and its implementation in **R**. 

## Intuition to scale equating

Previous chapter introduces the sources affecting the comparability of different poverty indices and presents the concept and empirical implementation of measurement invariance. In poverty research, households are ranked according to some indicators of (low) material and social deprivation using survey or census data. The indicators in question have a structure (unidimensional or multidimensional, See chapter \@ref(Chapter-1)) for the general population. One problem is that such a measurement model might not be adequate for a different population (such as countries) or for a different year. We saw in previous chapter that in statistical terms this means that the measurement model is not equivalent across groups. We now know that, once a MI assessment has been conducted, poverty can be compared on the factor using the alignment method. This, however, might not be fully satisfactory for policy makers as the values of a standardized latent variable make little practical sense. Furthermore, it is unclear how to use these values to set a poverty line. 

Another critical problem is that MI is adequate when scales have the same items and it does not solve the problem of working with scales that have different items or that have been upgraded in accordance with the living standards. Ideally, once measurement invariance is assessed researches would like to put everything into a meaningful metric and being able to compare measures that might have suffered from changes in its contents. Moreover, one question in poverty research is about how the severity of poverty is affected by changes in living standards and how this can be tractable using the available data. Thus, poverty measures need to be equated or equivalised in some way so that we can make meaningful comparisons across groups or time.

Scale or test equating is the answer from measurement theory to the problem of working with scales that are not necessarily equivalent across countries, time points, etc. The academic concern with making scores comparable has a history of more than 90 years [@Holland2007]. The modern development of the theory of scale equating can be traced back to the 1980s [@Haebara1980; @Petersen1983; @Kolen1988]. However, after the publication of @Kolen2004's seminal book, this framework has become more accessible through software development and implemented in several fields and has been under constant development [@Davier2010; @Gonzalez2017]. 

Educational testing has been the field at the forefront of scale equating. Admissions to universities often depend on the score of a test. But what if this test favours some students? and What if different versions of the tests are implemented? This poses a challenge for the admissions' office for two students might have different scores but just because the tests had different difficulties, i.e. one was easier than other. 

How does this problem translate to poverty measurement? There are two good examples to see this happening to our scales. According to @Townsend1979 poverty is relative across time and space. The *space* to identify the dimensions and indicators of poverty in the early 20th Century would look very different from the *space* of things and activities necessary to live with dignity in the 21st Century. For example, overcrowding and access to electricity were very good indicators of poverty in London but these days is no longer the case. That means that the two measure will have different deprivation indicators. So what does it mean that someone in the early 20th century had a deprivation score equal to 6 relative to someone that has a score equal to 3 today? Is it possible to compare how severely deprived they are? And how this might or might not impact the prevalence rate? 

This is a very crude example, but it helps to illustrate some of the challenges when comparing poverty overtime. However, in poverty research one common belief is that using the same indicators is sufficient to make valid comparisons across groups. Leaving aside the data collection and sampling issues associated with MI, we could think carefully about this assumption. Imagine that we use indicators of the early 20th to measure poverty today. Then we have two people with the same deprivation pattern- they lack electricity, cook with charcoal, and live in an overcrowded house. So they have the same deprivation scores. In the early 20th century it would not be very difficult to find such conditions in a small random sample. However, these days this situation would denote acute material deprivation. But these two people will have the same deprivation score, so without equating they would be equally deprived. How sensible is this conclusion? Well, not very. 

Scale equating aims at adjusting the differences in severity between tests and groups. The process of equating, given the response patterns, will notice that it is *more difficult* (as in Item Response Theory modelling) today being deprived of the items in question than 100 years ago. So, based on difficulty, the contemporary scale will be adjusted in terms of the early 20th Century's scale. If the equating process is successful, it should make sense and a score equal 3 today would mean something more severe in terms of a score equal 3 100 years ago, i.e. it will be higher. The inflation of today's score will be the result of equating and it will tell us a lot of useful information to adjust our conclusions about poverty and severity of deprivation. 

The second example focuses on the practical use of equating. Statistical offices tend to update their indices overtime, or we tend to have countries with two different measures but with some common indicators. So, they drop so indicators and include some new indicators. This just adds to the frustration of policymakers and academics because comparability is lost. One way to tackle this problem is scale equating. Figure \@(fig:changeEQ) displays a common situation in which the indicators have changed from measure A (Poverty) to measure B (Poverty B). There are seven items that appear in both measures and two items that were dropped and other two that were included. 

```{r changeEQ, echo = FALSE, message=FALSE, fig.align='center', fig.cap='This figure shows the case in which there is a modification in the contents of a measure. In red there are the common items between the two measures', fig.pos='H', fig.height=3, fig.width=3}
knitr::include_graphics("Diagram_EQ_1.png")
```

One crucial aspect of the example above is that if we have to tests with a common subset of items, then we can assess not only the effect of the updating of the scale but also we could link both measures and make them comparable across time points. We will see that for scale equating to work we need to identify as many **anchors** as we can, i.e. items that respect measurement invariance across tests. In this case we have seven potential candidates. In the following we introduce formally the concepts and methods to conduct scale equating. We will, of course, discuss some limitations of this approach as it might be sensible in all situations or desirable. 

## Theory of scale equating 

### Workflow in scale equating

Drawing upon @Davier2010, before formally introducing scale equating, we will present some of the stages involved in this process. 

1. Two or more poverty indices (test forms using the vocabulary in scale equating): We will have two indices and one research question: How to measure and compare the latent poverty levels of the population regardless of which index we use.

2. Detecting confounding differences: The task is to measure the latent poverty levels by avoiding the confounding effect of using indices that look at more or less acute deprivation indicators with the effect of latent poverty. This implies figuring out the differences between the estimated severities/difficulties of the indicators and the underlying latent level of poverty. 

3. Modelling the data generating process: Looking at different modelling alternatives, check their assumptions and assess how sensible using a given model is. There are several methods, observed score equating, IRT equating, kernel equating, etc. 

4. Error in equating: All models are wrong and thus it is vital to assess the extent to which the results of our model are useless or reasonable given the errors of the parameters that are being estimated. 

### Theory of IRT scale equating

In this edition of this book, we will focus on one of the most widely used form of equating: Item Response Theory (IRT) equating because it is a framework that has been used for reliability analysis and fits more naturally with the kind of thinking behind this book: Poverty is a concept and its measurement is based on reflective models where deprivation is a cause of poverty. Thus, people have a latent level of poverty that produces observed deprivation scores. 

Chapter \@ref(Chapter-2) underline the importance of explicitly working with models and blueprints in multidimensional poverty measurement. We proposed in equation \@ref(eq:mainmodel) a very simple model that looks as follows:

\begin{equation}
(\#eq:mainmodel)
\mathscr F = \{\mathscr X, F_{\theta} : \theta \in \Theta\}
\end{equation}

where the variables $x_1,...,x_n$ follow a certain distribution $F_{\theta}$, which is indexed by a parameter $\theta$ defined in the parameter space  $\Theta$. $\mathscr F$ is a family of all probability distributions on $\mathscr X$, which is just the set of all possible observed data.

We will borrow the formulation of @Gonzalez2017 notation to formulate the equating model based on Item Response Theory, as this will be the method that we will use in the book, as it is consistent with the methods and rationale that we have used so far in the book. 

For example, Item Response Theory (IRT) models, which are widely used in test equating, take the following general form:

\begin{equation}
(\#eq:irtmodel)
\mathscr F = {{0,1}, Bernoulli[\pi(\alpha_i, \omega_j)]:(\alpha_i, \omega_j) \in \mathbb{R} x \mathbb{R}}
\end{equation}

For a two-parameter IRT model we would have $x_{ij}$ denoting a binary deprivation indicator of an individual $i$ who poverty is measured based on the index $j$. That means that the probability of being deprived is given by: $(x_{ij} | \theta_i, a_j, b_j)  \sim Bernoulli$, where $\theta_i$ is person's latent poverty, $a_j$ is the discrimination of the item, $b_j$ is the severity of the item. \pi() is the item characteristic curve (ICC). 

The key parameters in this example are thus $\theta_i, a_j, b_j$. In the case that we have two indices $(j=2)$ we would like to link in some way the parameters of the first index ($X$) with the second index ($Y$). This could be achieved by estimating an IRT model for each index, extracting the parameters in question and then apply a linear equation to convert the IRT scores as:

\begin{equation}
(\#eq:irtequating1)
\theta_{yi} =  A\theta_{xi} + B
\end{equation}

To relate the parameters between the two tests we use the following:

\begin{equation}
(\#eq:irtequating2)
a_{yj} =  a_{xj} / A
\end{equation}

\begin{equation}
(\#eq:irtequating2)
b_{yj} =  Ab_{xj} + B
\end{equation}

where A and B are the equating coefficients and these need to be estimated using different methods such as mean-sigma, mean-geometric, Haebara, Stocking-Lord [@Gonzalez2017; Davier2010]. Once these parameters have been estimated, the next step consists in putting the index $X$ in terms of the index $Y$. This process assumes that scores are equated considering the latent level of poverty which relates to observed score. There are two perspectives to do so: Observed-score equating and True-score equating [@Kolen2004]. We explain these two very briefly:

Observed-score equating is based on the actual marginal score distributions (i.e. imagine a histogram of a deprivation score. This kind of equating uses the IRT model to adjust the score distributions across parallel forms, i.e. indices. To do so an observed score is calculated at each specified value of latent poverty level (remember that the metric of this variable is $\theta \sim (0,\sigma^2)$). This is summed/integrated across all levels of latent poverty to produce a marginal score distribution. The equipercentile is applied to establish the relationship between the two observed scores. 

True-score equating draws on the idea of a true score from Classical Test Theory (CTT). The whole idea is that people with the same level of latent poverty $\theta_j$ should have an equivalent true score, regardless the differences between tests. First, a latent poverty level ($\theta_j$, where $j=1$) is associated with a true score via the Newton-Rapson method. Then the true score of the A index is mapped into the index B using the same latent poverty level ($\theta_j$, where $j=2$). This procedure is often applied for each integer of the deprivation score. 


## Example of IRT equating with simulated data in R

To introduce IRT equating we will use two simulated indices (**A** and **B**) that capture a the simpliest situation in equating and in poverty measurement. Both indices are comprised of 15 binary indicators ($n=5000$). The first 11 indicators are the same in that they are measurement invariance and have the same severities and discrimination values. The remaining four indicators have different severities. The indicators of index A are more severe than the indicators of index B. This is a situation in which two very similar indices are applied to the same population -with no changes in the underlying level of poverty- but one is more severe than another. So it is not possible to compare the observed deprivation scores directly. This could be also depict a situation where two similar countries with similar living standards are compared using similar indices. 

```{r message=FALSE, warning=TRUE, include=FALSE}
test<-mplusObject(MONTECARLO = "NAMES=U1-U15; 
                  GENERATE = U1-U15(1);
                  CATEGORICAL = U1-U15;
                  GENCLASSES = c(2);
                  CLASSES = c(2);
                  NOBS = 5000;
                  SEED = 3454367;
                  NREP = 1;
                  SAVE = EQ_IRT_1_*.dat;
                  REPSAVE = all;", 
                  MODELPOPULATION ="%OVERALL%
                  
                  [c#1*2];
                  
                  f by u1@.9
                  u2@.9
                  u3@.9
                  u4@.9
                  u5@2.1
                  u6@1.9
                  u7@1.7
                  u8@1.5
                  u9@1.3
                  u10@1.1
                  u11@2.9
                  u12@2.7
                  u13@2.5
                  u14@2.3
                  u15@2.1;
                  
                  
                  
                  f@1;
                  
                  [u1$1@2.254]
                  [u2$1@1.125]
                  [u3$1@1.848]
                  [u4$1@1.867]
                  [u5$1@1.867]
                  [u6$1@1.234]
                  [u7$1@0.822]
                  [u8$1@2.368]
                  [u9$1@2.029]
                  [u10$1@2.689 ]
                  [u11$1@ 2.571]
                  [u12$1@2.75]
                  [u13$1@3.00]
                  [u14$1@3.25]
                  [u15$1@3.5];",
                  ANALYSIS = 
                    "TYPE = MIXTURE;
                  ALGORITHM=INTEGRATION;
                  PROCESS=8;")

for(i in 1:1){
  mplusModeler(test, modelout=paste("EQ_IRT_T1_DECREASE",i,".inp",sep=""),  writeData = "never",
               hashfilename = FALSE)
}
```

```{r message=FALSE, warning=TRUE, include=FALSE}
test<-mplusObject(MONTECARLO = "NAMES=U1-U15; 
                  GENERATE = U1-U15(1);
                  CATEGORICAL = U1-U15;
                  GENCLASSES = c(2);
                  CLASSES = c(2);
                  NOBS = 5000;
                  SEED = 345444;
                  NREP = 1;
                  SAVE = EQ_IRT_2_*.dat;
                  REPSAVE = all;", 
                  MODELPOPULATION ="%OVERALL%
                  
                  [c#1*2];
                  
                  f by u1@.84
                  u2@.84
                  u3@.84
                  u4@.84
                  u5@2.51
                  u6@1.96
                  u7@1.75
                  u8@1.55
                  u9@1.3
                  u10@1.6
                  u11@2.9
                  u12@2.8
                  u13@2.6
                  u14@2.5
                  u15@2.5;
                  
                  
                  
                  f@1;
                  
                   [u1$1@2.2]
                  [u2$1@1.1]
                  [u3$1@1.8]
                  [u4$1@1.8]
                  [u5$1@1.8]
                  [u6$1@1.2]
                  [u7$1@0.8]
                  [u8$1@2.3]
                  [u9$1@2.0]
                  [u10$1@2.9 ]
                  [u11$1@ 2.7]
                  [u12$1@2.0]
                  [u13$1@2.2]
                  [u14$1@2.5]
                  [u15$1@2.7];",
                  ANALYSIS = 
                    "TYPE = MIXTURE;
                  ALGORITHM=INTEGRATION;
                  PROCESS=8;")

for(i in 1:1){
  mplusModeler(test, modelout=paste("EQ_IRT_T2_DECREASE",i,".inp",sep=""),  writeData = "never",
               hashfilename = FALSE)
}

```


```{r message=FALSE, warning=FALSE, results="hide", include=FALSE}
runModels(filefilter = "EQ_IRT_T")
```

The data corresponding to index A is stored on the `EQ_IRT_1_1.dat` file and the data corresponding to index B is on the file `EQ_IRT_2_1.dat`. To familiarise ourselves with the data we will have a look at the files:

```{r}
A<-read.table("EQ_IRT_1_1.dat")
B<-read.table("EQ_IRT_2_1.dat")
```

We now tabulate the deprivation rates for each one of the 15 indicators so that we can appreciate the similarities and differences between the two samples. We see that there are just small variations in the first 11 items and larger variations for the remaining four. This is the expected behaviour given that we said that the latent levels of poverty are the same between samples A and B. 

```{r}
dep_propA<-unlist(lapply(A, function(x) mean(x)))
dep_propA<-round(dep_propA[1:15]*100,0)
dep_propA

dep_propB<-unlist(lapply(B, function(x) mean(x)))
dep_propB<-round(dep_propB[1:15]*100,0)
dep_propB
```

The first step in IRT-test equating consist in fitting and IRT model for each one of the two indices. If we think this carefully, this just a more formal way to compare the measurement properties of index A and B. That is, whether the indicators discriminate well between the poor and the not poor and what latent level of severity of poverty each indicator is capturing. 

The models will be fitted on **Mplus** via **R** using the `mplusAutomation()` function [@Hallquist2018]. We will write a more complex function because we will like to offer readers to run simulations to check the properties of equating, this is why we left and `i` in the commands. 

Here we fit the IRT model for both indices. We will create one script for each index (`EQ_IRT_1_1.inp` for index A for example): 

```{r message=FALSE, warning=FALSE, results="hide"}
test<-mplusObject(VARIABLE = "NAMES=V1-V15; 
                  USEVARIABLES=V1-V15; 
                  CATEGORICAL = V1-V15;", 
                  MODEL= "f by V1-V15*;
                                  f@1;")

for(i in 1:2){
  mplusModeler(test, modelout=paste("EQ_IRT_",i,"_1",".inp",sep=""),  writeData = "never",
               hashfilename = FALSE)
}
```

Now we request  `mplusAutomation()` to run both models on **Mplus** for us as follows: 

```{r message=FALSE, warning=FALSE, results="hide"}
for(i in 1:2){
runModels(paste("EQ_IRT_",i,"_1",".inp",sep=""))
}
```

Finally we read the models using the `readModels()` function. We will store the files in the objects `irt_1` and `irt_2` and then we will put them in a list so that we can put them in the correct format for the equating (i.e. the irt parameters *a, b and c* of each item need to be in columns). So, using `lapply()` we can put them in the correct format and store them on the `irtS` list. 

```{r}
irt_1<-readModels(filefilter ="EQ_IRT_1")
irt_2<-readModels(filefilter ="EQ_IRT_2")

irtS<-list(irt_1,irt_2)


irtS<-lapply(irtS, function(x) {
  x<-x$parameters$irt.parameterization
  x<-x[1:30,]
  x<-data.frame(a=x$est[1:15],b=x$est[16:30],c=0)
  x
}
)
```

The `irtS` list contains the paremets of indices A and B. We will rename them to facilitate the specification using the excellent `SNSequate` package [@Gonzalez2014]. We need to tell `SNSequate` the dataframes containing the IRT-parameters of each test and also which items are the *anchors* -common items-. 


```{r message=FALSE, warning=FALSE}
library(SNSequate)
parm.a<-irtS[[1]]
parm.b<-irtS[[2]]
comitems<-(1:11)
parm <- cbind(parm.a, parm.b)
```

The key function to conduct IRT-equating is `irt.link()`, this function uses the parameters, takes into account the common items and considers which kind of IRT we want to use, in this case a two-parameter model. In this case we will use 1.7 for the constant D, as it is common practice. 

```{r message=FALSE, warning=FALSE}
eqconst<-irt.link(parm, comitems, model = "2PL", icc = "logistic", D = 1.7)
```

The object `eqconst` contains the estimated constants for equating: a and b. It will estimate these constants using different methods (mean-mean, StockLord, Haebara). For this example, we will use the constants from the StockLord method. We simply extract the constants as follows: 

```{r}
Sl_a<-eqconst$StockLord[1]  
Sl_b<-eqconst$StockLord[2] 
```

The next step is to apply the contants to the IRT coefficients. What we need to do is to rescale the irt-parameters of test A -because we are equating test A with B- using the two constants as follows: 

```{r message=FALSE, warning=FALSE}
irtS[[1]]$a<-irtS[[1]]$a/Sl_a
irtS[[1]]$b<-Sl_a*(irtS[[1]]$b)+Sl_b
irtS[[1]]
```

Now we have equivalent IRT-parameters so the latent scores can be fully compared. Section XX described two types of equating: observed and true. We will impliement both below. For the true scale equating we simply say the names of the objects with the IRT-parameters, the method and we will use the following defaults for the scalling parameters $A=1$ and $B=0$. We also specify the common items. We store the results on the `true-eqscale` object. For the observed equating we need to specify the `theta_points`, i.e. the standarised values of the latent poverty level. Then we apply the function changing the method and adding the theta points. We store the output on the `obs_eqscale` object. 

```{r message=FALSE, warning=FALSE}
theta_points=c(-5,-4,-3,-2,-1,0,1,2,3,4,5)

true_eqscale<-irt.eq(15, irtS[[1]], irtS[[2]], method="TS", A=1, B=0, common=comitems)
obs_eqscale<-irt.eq(15, irtS[[1]], irtS[[2]], theta_points, method="OS", common=comitems,
                                             A=1, B=0)

```

We now do some manipulations to the `true_mean` object to extract the equated scores of form A in terms of B (`true_eqscale$tau_y`)  and the latent values of poverty for each equated score (`true_eqscale$theta_equivalent`). We create a simple table to compare the equated score A in terms of B. We observed that the values of A in terms of B tend to be higher. Why is that? Remeber that A had more severe indicators. That means that being deprived under test B was less severe. The number of deprivations in test A and B do not reflect the same severity levels, the latent severity under test A requires a higher observed deprivation score.  

Sometimes is tricky to interpret these findings are there are double negatives. One way to think about this is by thinking in terms of an extreme situation. Imagine that test A is way more severe than test B, for example, test A uses indicators like having dirt floor, lacking electricity, unsafe source of water, walls made from cardboard to identify deprivation relative to B that uses indicators like floor without tiles, cant' afford electricity, clean water connected to the property, lacking walls made from cement or bricks. For the same living standards, a score of 4 in test A denotes a more extreme situation than a score equal to 4 in test B. Thus, the same severity of A in terms of B should be way higher. In the current example we have something similar but less dramatic. 

```{r message=FALSE, warning=FALSE}
true_mean<-data.frame(Score.AintermsofB=true_eqscale$tau_y, Latent=true_eqscale$theta_equivalent)
true_mean$ScoreB<-0:15
true_mean
```

Now we will do the same extraction process for the observed score. We extract the values of the A score in terms of B (`obs_eqscale$e_Y_x`). We find that we get similar results. The observed score of A in terms of B is higher than the score of index B. 

```{r message=FALSE, warning=FALSE}
obs_mean<-data.frame(Score.AintermsofB=obs_eqscale$e_Y_x, obs_mean=c(0:15))
obs_mean
```

To see these findings from a different perspective, we will plot the results. The first plot, uses a 45 degree line to denote a situation where scores A and B are the same, i.e. a score of 3 measures the same underlying level of poverty. The pink line is the score A in terms of B. We see that the pink line is almmost always below the 45 degree line. This means that the rescaled score A is always higher than B. For the same severity, someone assessed using index A has a higher observed deprivation score. 

```{r message=FALSE, warning=FALSE}
library(ggplot2)

ggplot(true_mean,aes(Score.AintermsofB,ScoreB)) + geom_line(size=2, color="pink") +  xlab("Measure A (scaled in B-terms)") + ylab("Measure B") + theme_bw() +  geom_abline(intercept = 0, slope = 1) +
  scale_y_continuous( limits = c(-.1,15), expand = c(0,0), breaks = seq(1, 15, 1) ) + 
  scale_x_continuous( limits = c(-.1,15), expand = c(0,0), breaks = seq(1, 15, 1)) 
```

```{r message=FALSE, warning=FALSE, results="hide", include=FALSE}
jpeg("EQ_plot1.jpg", units="cm", width=10, height=10, res=300)
ggplot(true_mean,aes(Score.AintermsofB,ScoreB)) + geom_line(size=2, color="pink") +  xlab("Measure A (scaled in B-terms)") + ylab("Measure B") + theme_bw() +  geom_abline(intercept = 0, slope = 1) +
  scale_y_continuous( limits = c(-.1,15), expand = c(0,0), breaks = seq(1, 15, 1) ) + 
  scale_x_continuous( limits = c(-.1,15), expand = c(0,0), breaks = seq(1, 15, 1)) 
dev.off()
```


```{r message=FALSE, warning=FALSE, results="hide", include=FALSE}
library(reshape2)

mm<-melt(true_mean, id="Latent")

ggplot(mm,aes(x=Latent,y=value, group=variable, colour=variable)) + geom_line( size=2) + 
  xlab("Latent Poverty") + ylab("Scales") + theme_bw() 

```


## Real-data example


